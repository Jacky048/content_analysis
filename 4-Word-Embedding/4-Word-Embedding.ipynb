{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 - Word Embeddings\n",
    "\n",
    "This week, we build on last week's topic modeling techniques by taking a text corpus we have developed, specifying an underlying number of dimensions, and training a model with a neural network auto-encoder (one of Google's word2vec  algorithms) that best describes corpus words in their local linguistic contexts, and exploring their locations in the resulting space to learn about the discursive culture that produced them. Documents here are represented as densely indexed locations in dimensions, rather than sparse mixtures of topics (as in LDA topic modeling), so that distances between those documents (and words) are consistently superior, though they require the full vector of dimension loadings (rather than just a few selected topic loadings) to describe. We will explore these spaces to understand complex, semantic relationships between words, index documents with descriptive words, identify the likelihood that a given document would have been produced by a given vector model, and explore how semantic categories can help us understand the cultures that produced them.\n",
    "\n",
    "For this notebook we will be using the following packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#All these packages need to be installed from pip\n",
    "import gensim#For word2vec, etc\n",
    "import requests #For downloading our datasets\n",
    "import nltk #For stop words and stemmers\n",
    "import numpy as np #For arrays\n",
    "import pandas #Gives us DataFrames\n",
    "import matplotlib.pyplot as plt #For graphics\n",
    "import seaborn #Makes the graphics look nicer\n",
    "import sklearn.metrics.pairwise #For cosine similarity\n",
    "import sklearn.manifold #For T-SNE\n",
    "import sklearn.decomposition #For PCA\n",
    "\n",
    "#This 'magic' command makes the plots work better\n",
    "#in the notebook, don't use it outside of a notebook.\n",
    "#Also you can ignore the warning\n",
    "%matplotlib inline\n",
    "\n",
    "import os #For looking through files\n",
    "import os.path #For managing file paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting our corpora\n",
    "\n",
    "Instead of downloading our corpora, we have download them in advance; a subset of the [senate press releases](https://github.com/lintool/GrimmerSenatePressReleases) are in `/mnt/efs/resources/shared/Notebook-4-data/grimmerPressReleases`. We will load them into a DataFrame, but first we need to define a function to convert directories of text files into DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadDir(targetDir, category):\n",
    "    allFileNames = os.listdir(targetDir)\n",
    "    #We need to make them into useable paths and filter out hidden files\n",
    "    filePaths = [os.path.join(targetDir, fname) for fname in allFileNames if fname[0] != '.']\n",
    "\n",
    "    #The dict that will become the DataFrame\n",
    "    senDict = {\n",
    "        'category' : [category] * len(filePaths),\n",
    "        'filePath' : [],\n",
    "        'text' : [],\n",
    "    }\n",
    "\n",
    "    for fPath in filePaths:\n",
    "        with open(fPath) as f:\n",
    "            senDict['text'].append(f.read())\n",
    "            senDict['filePath'].append(fPath)\n",
    "\n",
    "    return pandas.DataFrame(senDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the function in all the directories in `/mnt/efs/resources/shared/Notebook-4-data/grimmerPressReleases`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataDir = '/mnt/efs/resources/shared/Notebook-4-data/grimmerPressReleases'\n",
    "\n",
    "senReleasesDF = pandas.DataFrame()\n",
    "\n",
    "for senatorName in [d for d in os.listdir(dataDir) if d[0] != '.']:\n",
    "    senPath = os.path.join(dataDir, senatorName)\n",
    "    senReleasesDF = senReleasesDF.append(loadDir(senPath, senatorName), ignore_index = True)\n",
    "\n",
    "senReleasesDF[:100:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to remove stop words and stem. Tokenizing requires two steps. Word2Vec needs to retain the sentence structure so as to capture a \"continuous bag of words (CBOW)\" and all of the skip-grams within a word window. The algorithm tries to preserve the distances induced by one of these two local structures. This is very different from clustering and LDA topic modeling which extract unordered words alone. As such, tokenizing is slightly more involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normlizeTokens(tokenLst, stopwordLst = None, stemmer = None, lemmer = None, vocab = None):\n",
    "    #We can use a generator here as we just need to iterate over it\n",
    "\n",
    "    #Lowering the case and removing non-words\n",
    "    workingIter = (w.lower() for w in tokenLst if w.isalpha())\n",
    "\n",
    "    #Now we can use the semmer, if provided\n",
    "    if stemmer is not None:\n",
    "        workingIter = (stemmer.stem(w) for w in workingIter)\n",
    "\n",
    "    #And the lemmer\n",
    "    if lemmer is not None:\n",
    "        workingIter = (lemmer.lemmatize(w) for w in workingIter)\n",
    "\n",
    "    #And remove the stopwords\n",
    "    if stopwordLst is not None:\n",
    "        workingIter = (w for w in workingIter if w not in stopwordLst)\n",
    "        \n",
    "    #We will return a list with the stopwords removed\n",
    "    if vocab is not None:\n",
    "        vocab_str = '|'.join(vocab)\n",
    "        workingIter = (w for w in workingIter if re.match(vocab_str, w))\n",
    "    \n",
    "    return list(workingIter)\n",
    "\n",
    "#initialize our stemmer and our stop words\n",
    "stop_words_nltk = nltk.corpus.stopwords.words('english')\n",
    "snowball = nltk.stem.snowball.SnowballStemmer('english')\n",
    "wordnet = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Apply our functions, notice each row is a list of lists now\n",
    "senReleasesDF['tokenized_sents'] = senReleasesDF['text'].apply(lambda x: [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])\n",
    "senReleasesDF['normalized_sents'] = senReleasesDF['tokenized_sents'].apply(lambda x: [normlizeTokens(s, stopwordLst = stop_words_nltk, stemmer = None) for s in x])\n",
    "\n",
    "senReleasesDF[:100:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "We will be using the gensim implementation of [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec).\n",
    "\n",
    "To load our data our data we give all the sentences to the trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "senReleasesW2V = gensim.models.word2vec.Word2Vec(senReleasesDF['normalized_sents'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the word2vec object the words each have a vector. To access the vector directly, use the square braces (`__getitem__`) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "senReleasesW2V['president'][:10] #Shortening because it's very large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want the full matrix, `syn0` stores all the vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "senReleasesW2V.syn0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, `index2word` lets you translate from the matrix to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "senReleasesW2V.index2word[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at a few things that come from the word vectors. The first is to find similar vectors (cosine similarity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "senReleasesW2V.most_similar('president')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "senReleasesW2V.most_similar('war')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find which word least matches the others within a word set (cosine similarity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "senReleasesW2V.doesnt_match(['administration', 'administrations', 'presidents', 'president', 'washington'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find which word best matches the result of a semantic *equation* (here, we seek the words whose vectors best fit the missing entry from the equation: **X + Y - Z = _**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "senReleasesW2V.most_similar(positive=['clinton', 'republican'], negative = ['democrat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that **Clinton + Republican - Democrat = Bush**. In other words, in this dataset, **Clinton** is to **Democrat** as **Bush** is to **Republican**. Whoah!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also save the vectors for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "senReleasesW2V.save(\"data/senpressreleasesWORD2Vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use dimension reduction to visulize the vectors. We will start by selecting a subset we want to plot. Let's look at the top words from the set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numWords = 50\n",
    "targetWords = senReleasesW2V.index2word[:numWords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then extract their vectors and create our own smaller matrix that preserved the distances from the original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordsSubMatrix = []\n",
    "for word in targetWords:\n",
    "    wordsSubMatrix.append(senReleasesW2V[word])\n",
    "wordsSubMatrix = np.array(wordsSubMatrix)\n",
    "wordsSubMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use PCA to reduce the dimesions (e.g., to 50), and T-SNE to project them down to the two we will visualize. We note that this is nondeterministic process, and so you can repeat and achieve alternative projectsions/visualizations of the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pcaWords = sklearn.decomposition.PCA(n_components = 50).fit(wordsSubMatrix)\n",
    "reducedPCA_data = pcaWords.transform(wordsSubMatrix)\n",
    "#T-SNE is theoretically better, but you should experiment\n",
    "tsneWords = sklearn.manifold.TSNE(n_components = 2).fit_transform(reducedPCA_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now can plot the points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_frame_on(False)\n",
    "plt.scatter(tsneWords[:, 0], tsneWords[:, 1], alpha = 0)#Making the points invisible \n",
    "for i, word in enumerate(targetWords):\n",
    "    ax.annotate(word, (tsneWords[:, 0][i],tsneWords[:, 1][i]), size =  20 * (numWords - i) / numWords)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My visualization above puts ``said`` next to ``congress`` and ``bill`` near ``act``. ``health`` is beside ``care`` and ``national`` abuts ``security``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Your Turn*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that build a word2vec model with your corpus. Interrogate word relationships in the resulting space. Plot a subset of your words. What do these word relationships reveal about the *social* and *cultural game* underlying your corpus? What was surprising--what violated your prior understanding of the corpus? What was expected--what confirmed your knowledge about this domain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Instead of just looking at just how words embed within in the space, we can look at how the different documents relate to each other within the space. First lets load our data--abstracts of most U.S. physics papers from the 1950s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "apsDF = pandas.read_csv('/mnt/efs/resources/shared/Notebook-4-data/APSabstracts1950s.csv', index_col = 0)\n",
    "apsDF[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load these as documents into Word2Vec, but first we need to normalize and pick some tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keywords = ['photomagnetoelectric', 'quantum', 'boltzmann', 'proton', 'positron', 'feynman', 'classical', 'relativity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "apsDF['tokenized_words'] = apsDF['abstract'].apply(lambda x: nltk.word_tokenize(x))\n",
    "apsDF['normalized_words'] = apsDF['tokenized_words'].apply(lambda x: normlizeTokens(x, stopwordLst = stop_words_nltk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "taggedDocs = []\n",
    "for index, row in apsDF.iterrows():\n",
    "    #Just doing a simple keyword assignment\n",
    "    docKeywords = [s for s in keywords if s in row['normalized_words']]\n",
    "    docKeywords.append(row['copyrightYear'])\n",
    "    docKeywords.append(row['doi']) #This lets us extract individual documnets since doi's are unique\n",
    "    taggedDocs.append(gensim.models.doc2vec.LabeledSentence(words = row['normalized_words'], tags = docKeywords))\n",
    "apsDF['TaggedAbstracts'] = taggedDocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train a Doc2Vec model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "apsD2V = gensim.models.doc2vec.Doc2Vec(apsDF['TaggedAbstracts'], size = 100) #Limiting to 100 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get vectors for the tags/documents, just as we did with words. Documents are actually the centroids (high dimensional average points) of their words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "apsD2V.docvecs[1952]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words can still be accessed in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "apsD2V['atom']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can still use the ``most_similar`` command to perform simple semantic equations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "apsD2V.most_similar(positive = ['atom','electrons'], negative = ['electron'], topn = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is interesting. **Electron** is to **electrons** as **atom** is to **atoms**. Another way to understand this, developed below is: **electrons - electron** induces a singular to plural dimension, so when we subtract **electron** from **atom** and add **electrons**, we get **atoms**! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "apsD2V.most_similar(positive = ['einstein','law'], negative = ['equation'], topn = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words **Einstein** minus **equation** plus **law** equals **Meissner**--Walthur Meissner studied mechanical engineering and physics ... and was more likely to produce a \"law\" than a \"equation\", like the Meissner effect, the damping of the magnetic field in superconductors. If we built our word-embedding with a bigger corpus like the entire arXiv, a massive repository of physics preprints, we would see many more such relationships like **gravity - Newton + Einstein = relativity**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute all of these *by hand*--explicitly wth vector algebra: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sklearn.metrics.pairwise.cosine_similarity(apsD2V['electron'].reshape(1,-1), apsD2V['positron'].reshape(1,-1))\n",
    "#We reorient the vectors with .reshape(1, -1) so that they can be computed without a warning in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the doc2vec model, the documents have vectors just as the words do, so that we can compare documents with each other and also with words (similar to how a search engine locates a webpage with a query). First, we will calculate the distance between a word and documents in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "apsD2V.docvecs.most_similar([ apsD2V['electron'] ], topn=5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we search for the first of these on the web (these are doi codes), we find the following...a pretty good match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"PhysRev.98.875.jpg\", width=1000, height=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go the other way around and find words most similar to this document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "apsD2V.most_similar( [ apsD2V.docvecs['10.1103/PhysRev.98.875'] ], topn=5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even look for documents most like a query composed of multiple words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "apsD2V.docvecs.most_similar([ apsD2V['electron']+apsD2V['positron']+apsD2V['neutron']], topn=5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot some words and documents against one another with a heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "heatmapMatrix = []\n",
    "for tagOuter in keywords:\n",
    "    column = []\n",
    "    tagVec = apsD2V.docvecs[tagOuter].reshape(1, -1)\n",
    "    for tagInner in keywords:\n",
    "        column.append(sklearn.metrics.pairwise.cosine_similarity(tagVec, apsD2V.docvecs[tagInner].reshape(1, -1))[0][0])\n",
    "    heatmapMatrix.append(column)\n",
    "heatmapMatrix = np.array(heatmapMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "heatmapMatrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "hmap = ax.pcolor(heatmapMatrix, cmap='terrain')\n",
    "cbar = plt.colorbar(hmap)\n",
    "\n",
    "cbar.set_label('cosine similarity', rotation=270)\n",
    "a = ax.set_xticks(np.arange(heatmapMatrix.shape[1]) + 0.5, minor=False)\n",
    "a = ax.set_yticks(np.arange(heatmapMatrix.shape[0]) + 0.5, minor=False)\n",
    "\n",
    "a = ax.set_xticklabels(keywords, minor=False, rotation=270)\n",
    "a = ax.set_yticklabels(keywords, minor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at a heatmap of similarities between the first ten documents in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "targetDocs = apsDF['doi'][:10]\n",
    "\n",
    "heatmapMatrixD = []\n",
    "\n",
    "for tagOuter in targetDocs:\n",
    "    column = []\n",
    "    tagVec = apsD2V.docvecs[tagOuter].reshape(1, -1)\n",
    "    for tagInner in targetDocs:\n",
    "        column.append(sklearn.metrics.pairwise.cosine_similarity(tagVec, apsD2V.docvecs[tagInner].reshape(1, -1))[0][0])\n",
    "    heatmapMatrixD.append(column)\n",
    "heatmapMatrixD = np.array(heatmapMatrixD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "hmap = ax.pcolor(heatmapMatrixD, cmap='terrain')\n",
    "cbar = plt.colorbar(hmap)\n",
    "\n",
    "cbar.set_label('cosine similarity', rotation=270)\n",
    "a = ax.set_xticks(np.arange(heatmapMatrixD.shape[1]) + 0.5, minor=False)\n",
    "a = ax.set_yticks(np.arange(heatmapMatrixD.shape[0]) + 0.5, minor=False)\n",
    "\n",
    "a = ax.set_xticklabels(targetDocs, minor=False, rotation=270)\n",
    "a = ax.set_yticklabels(targetDocs, minor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at a heatmap of similarities between the first ten documents and our keywords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "heatmapMatrixC = []\n",
    "\n",
    "for tagOuter in targetDocs:\n",
    "    column = []\n",
    "    tagVec = apsD2V.docvecs[tagOuter].reshape(1, -1)\n",
    "    for tagInner in keywords:\n",
    "        column.append(sklearn.metrics.pairwise.cosine_similarity(tagVec, apsD2V.docvecs[tagInner].reshape(1, -1))[0][0])\n",
    "    heatmapMatrixC.append(column)\n",
    "heatmapMatrixC = np.array(heatmapMatrixC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "hmap = ax.pcolor(heatmapMatrixC, cmap='terrain')\n",
    "cbar = plt.colorbar(hmap)\n",
    "\n",
    "cbar.set_label('cosine similarity', rotation=270)\n",
    "a = ax.set_xticks(np.arange(heatmapMatrixC.shape[1]) + 0.5, minor=False)\n",
    "a = ax.set_yticks(np.arange(heatmapMatrixC.shape[0]) + 0.5, minor=False)\n",
    "\n",
    "a = ax.set_xticklabels(keywords, minor=False, rotation=270)\n",
    "a = ax.set_yticklabels(targetDocs, minor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will save the model in case we would like to use it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "apsD2V.save('data/apsW2V')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can later load it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#apsD2V = gensim.models.word2vec.Word2Vec.load('data/apsW2V')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Your Turn*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that build a doc2vec model with your corpus. Interrogate document and word relationships in the resulting space. Construct a heatmap that plots the distances between a subset of your documents against each other, and against a set of informative words. Find distances between *every* document in your corpus and a word or query of interest. What do these doc-doc proximities reveal about your corpus? What do these word-doc proximities highlight? Demonstrate and document one reasonable way to select a defensible subset of query-relevant documents for subsequent analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# The Score Function\n",
    "\n",
    "The score function is a simple calculation developed by [Matt Taddy](https://arxiv.org/pdf/1504.07295.pdf) to calculate the likelihood that a given text would have been generated by a word-embedding model by summing the inner product between each pair of the text's word vectors. \n",
    "\n",
    "Here, we explore this using a model trained with millions of resumes from the CareerBuilder website (we can't share the private resumes...but we can share a model built with them :-):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resume_model  = gensim.models.word2vec.Word2Vec.load('/mnt/efs/resources/shared/Notebook-4-data/resume.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the vacabularies of this model by building a word-index map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = resume_model.index2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's load a few job ads. Here, we only use a small sample of all of them. Uncomment this cell if you want to load more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with open('data/joblistings.merged.parsed.unique.grpbyyear.2010-2015.02.tsv','r') as tsv:\n",
    "#     ads = [line.strip().split('\\t') for line in tsv]\n",
    "    \n",
    "# adsDF = pandas.DataFrame(ads, columns = ads[0])\n",
    "# reducedDF = adsDF[['hiringOrganization_organizationName', 'jobDescription', 'jobLocation_address_region', 'jobLocation_geo_latitude', 'jobLocation_geo_longitude', 'qualifications', 'responsibilities']][1:]\n",
    "# N = reducedDF.shape[0]\n",
    "# indices = random.sample(range(1, N+1), 100)\n",
    "# sampleDF = reducedDF.iloc[indices]\n",
    "# sampleDF.to_csv('data/SampleJobAds.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just load the sample and take a look at it. The sentences in each job description are already tokenized and normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>hiringOrganization_organizationName</th>\n",
       "      <th>jobDescription</th>\n",
       "      <th>jobLocation_address_region</th>\n",
       "      <th>jobLocation_geo_latitude</th>\n",
       "      <th>jobLocation_geo_longitude</th>\n",
       "      <th>qualifications</th>\n",
       "      <th>responsibilities</th>\n",
       "      <th>tokenized_sents</th>\n",
       "      <th>normalized_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>158844</td>\n",
       "      <td>Golfsmith International, Inc.</td>\n",
       "      <td>\"Sales Associate Tracking Code 220425-971 Job ...</td>\n",
       "      <td>California</td>\n",
       "      <td>33.91918</td>\n",
       "      <td>-118.41647</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Ensure each Customer receives exceptional ser...</td>\n",
       "      <td>[[``, Sales, Associate, Tracking, Code, 220425...</td>\n",
       "      <td>[[sales, associate, tracking, code, job, descr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>257645</td>\n",
       "      <td>Intel</td>\n",
       "      <td>For PHY system engineering team within the Wir...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[For, PHY, system, engineering, team, within,...</td>\n",
       "      <td>[[for, phy, system, engineering, team, within,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>107875</td>\n",
       "      <td>Florida Hospital</td>\n",
       "      <td>*RN Medical Oncology PCU Orlando - Nights* Flo...</td>\n",
       "      <td>Florida</td>\n",
       "      <td>28.53834</td>\n",
       "      <td>-81.37924</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[*RN, Medical, Oncology, PCU, Orlando, -, Nig...</td>\n",
       "      <td>[[medical, oncology, pcu, orlando, florida, ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>202394</td>\n",
       "      <td>Hitachi Data Systems</td>\n",
       "      <td>Title: Specialist Sales Account Representative...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Title, :, Specialist, Sales, Account, Repres...</td>\n",
       "      <td>[[title, specialist, sales, account, represent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>109675</td>\n",
       "      <td>Footprint Retail Services</td>\n",
       "      <td>**Footprint Retail Services** **Job Descriptio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A Merchandiser must complete all assigned merc...</td>\n",
       "      <td>[[**Footprint, Retail, Services**, **Job, Desc...</td>\n",
       "      <td>[[retail, job, title, retail, merchandiser, re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>215973</td>\n",
       "      <td>Home Depot</td>\n",
       "      <td>Position Purpose: Provide outstanding service ...</td>\n",
       "      <td>Indiana</td>\n",
       "      <td>41.13060</td>\n",
       "      <td>-85.12886</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Provide outstanding service to ensure efficien...</td>\n",
       "      <td>[[Position, Purpose, :, Provide, outstanding, ...</td>\n",
       "      <td>[[position, purpose, provide, outstanding, ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>207524</td>\n",
       "      <td>Home Depot</td>\n",
       "      <td>The Asset Protection Specialist is primarily r...</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>40.21455</td>\n",
       "      <td>-74.61932</td>\n",
       "      <td>Must be eighteen years of age or older. Must p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[The, Asset, Protection, Specialist, is, prim...</td>\n",
       "      <td>[[the, asset, protection, specialist, is, prim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>64426</td>\n",
       "      <td>East West Bank</td>\n",
       "      <td># Job Description East West Bank is one of the...</td>\n",
       "      <td>California</td>\n",
       "      <td>34.06862</td>\n",
       "      <td>-118.02757</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We are currently seeking a Customer Service Ce...</td>\n",
       "      <td>[[#, Job, Description, East, West, Bank, is, o...</td>\n",
       "      <td>[[job, description, east, west, bank, is, one,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>245192</td>\n",
       "      <td>IBM</td>\n",
       "      <td>Job Description IBM is seeking to hire a Senio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Job, Description, IBM, is, seeking, to, hire...</td>\n",
       "      <td>[[job, description, ibm, is, seeking, to, hire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>202429</td>\n",
       "      <td>Hitachi Data Systems</td>\n",
       "      <td>Title: Field Solutions Engineer Location: New ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Job Functions;Specific duties in this role wil...</td>\n",
       "      <td>[[Title, :, Field, Solutions, Engineer, Locati...</td>\n",
       "      <td>[[title, field, solutions, engineer, location,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>269503</td>\n",
       "      <td>J&amp;J Family of Companies</td>\n",
       "      <td>Project Manager (m/w) - Government &amp; Public Af...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Project, Manager, (, m/w, ), -, Government, ...</td>\n",
       "      <td>[[project, manager, government, public, jansse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>139164</td>\n",
       "      <td>Genesis Healthcare</td>\n",
       "      <td>Certified Medicine Aide Area of Interest: Nurs...</td>\n",
       "      <td>Kansas</td>\n",
       "      <td>39.10972</td>\n",
       "      <td>-95.08775</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Receives report at the beginning of shift with...</td>\n",
       "      <td>[[Certified, Medicine, Aide, Area, of, Interes...</td>\n",
       "      <td>[[certified, medicine, aide, area, of, interes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>255915</td>\n",
       "      <td>Ingersoll Rand</td>\n",
       "      <td>**Description:** At Ingersoll Rand we're passi...</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>35.14953</td>\n",
       "      <td>-90.04898</td>\n",
       "      <td>Considerable knowledge of industry related sup...</td>\n",
       "      <td>Ingersoll Rand is a diverse and inclusive envi...</td>\n",
       "      <td>[[**Description, :, **, At, Ingersoll, Rand, w...</td>\n",
       "      <td>[[at, ingersoll, rand, we, passionate, about, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>173294</td>\n",
       "      <td>HamiltonConstructionCompany</td>\n",
       "      <td>\"Hamilton Construction,in businesssince 1939 a...</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>44.04624</td>\n",
       "      <td>-123.02203</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[``, Hamilton, Construction, ,, in, businesss...</td>\n",
       "      <td>[[hamilton, construction, in, businesssince, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>116855</td>\n",
       "      <td>G6 Hospitality</td>\n",
       "      <td>Business Unit: DirectEmployers Title: Manager ...</td>\n",
       "      <td>Texas</td>\n",
       "      <td>32.83707</td>\n",
       "      <td>-97.08195</td>\n",
       "      <td>High School diploma or equivalent;Computer pro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Business, Unit, :, DirectEmployers, Title, :...</td>\n",
       "      <td>[[business, unit, directemployers, title, mana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>40701</td>\n",
       "      <td>Dollar Tree</td>\n",
       "      <td>Auto req ID 41872BR Title ASSISTANT MANAGER Em...</td>\n",
       "      <td>Texas</td>\n",
       "      <td>32.85791</td>\n",
       "      <td>-97.25474</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Auto, req, ID, 41872BR, Title, ASSISTANT, MA...</td>\n",
       "      <td>[[auto, req, id, title, assistant, manager, em...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>292406</td>\n",
       "      <td>Johns Hopkins Medicine</td>\n",
       "      <td>Johns Hopkins employs more than 20,000 people ...</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>39.29038</td>\n",
       "      <td>-76.61219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Johns, Hopkins, employs, more, than, 20,000,...</td>\n",
       "      <td>[[johns, hopkins, employs, more, than, people,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>358904</td>\n",
       "      <td>LHC Group</td>\n",
       "      <td>Job Description As a CNA, you will perform a v...</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>34.74481</td>\n",
       "      <td>-87.66753</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Your specific duties for this role will includ...</td>\n",
       "      <td>[[Job, Description, As, a, CNA, ,, you, will, ...</td>\n",
       "      <td>[[job, description, as, a, cna, you, will, per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>126507</td>\n",
       "      <td>GE</td>\n",
       "      <td>2306307 **Business** GE Capital **Business Seg...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Doswiadczenie w pracy na podobnym stanowisku;W...</td>\n",
       "      <td>Prowadzenie projektow zw\\. z rozwojem pracowni...</td>\n",
       "      <td>[[2306307, **Business**, GE, Capital, **Busine...</td>\n",
       "      <td>[[ge, capital, capital, international, bank, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>90538</td>\n",
       "      <td>EY</td>\n",
       "      <td>Title: Greater China TAS Referrals (China Sout...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Title, :, Greater, China, TAS, Referrals, (,...</td>\n",
       "      <td>[[title, greater, china, tas, referrals, china...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>59256</td>\n",
       "      <td>Dr Pepper Snapple Group</td>\n",
       "      <td>Merchandiser The Merchandiser is responsible f...</td>\n",
       "      <td>Texas</td>\n",
       "      <td>29.76328</td>\n",
       "      <td>-95.36327</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Merchandiser, The, Merchandiser, is, respons...</td>\n",
       "      <td>[[merchandiser, the, merchandiser, is, respons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>85716</td>\n",
       "      <td>Eurofins Lancaster Laboratories</td>\n",
       "      <td>### Eurofins is the world leader in the food, ...</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>39.55895</td>\n",
       "      <td>-84.30411</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[#, #, #, Eurofins, is, the, world, leader, i...</td>\n",
       "      <td>[[eurofins, is, the, world, leader, in, the, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>203342</td>\n",
       "      <td>HMSHOST</td>\n",
       "      <td>\"**Having the right ingredients makes all the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Greets customers and takes food order provide...</td>\n",
       "      <td>[[``, **Having, the, right, ingredients, makes...</td>\n",
       "      <td>[[the, right, ingredients, makes, all, the, di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>283327</td>\n",
       "      <td>Jewel-Osco</td>\n",
       "      <td>JOB: Retail Clerk  Liquor/Beer/Wine JOB OVERVI...</td>\n",
       "      <td>Montana</td>\n",
       "      <td>48.19579</td>\n",
       "      <td>-114.31291</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sells, receives, stocks and may order beer/liq...</td>\n",
       "      <td>[[JOB, :, Retail, Clerk, Liquor/Beer/Wine, JOB...</td>\n",
       "      <td>[[job, retail, clerk, job, overview, provides,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>320876</td>\n",
       "      <td>KeyPoint Government Solutions</td>\n",
       "      <td>**Overview:** KeyPoint Government Solutions is...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[**Overview, :, **, KeyPoint, Government, Sol...</td>\n",
       "      <td>[[keypoint, government, solutions, is, current...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>311069</td>\n",
       "      <td>Kelly Services</td>\n",
       "      <td>**Strategic Sales Lead  Premier Brands and Con...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bachelors degree (MBA preferred) or equivalent...</td>\n",
       "      <td>This is a field based corporate sales position...</td>\n",
       "      <td>[[**Strategic, Sales, Lead, Premier, Brands, a...</td>\n",
       "      <td>[[sales, lead, premier, brands, and, consumer,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>141835</td>\n",
       "      <td>Genesis Rehabilitation</td>\n",
       "      <td>Job Title: Occupational Therapist Area of Inte...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Screens, examines and evaluates patients, incl...</td>\n",
       "      <td>[[Job, Title, :, Occupational, Therapist, Area...</td>\n",
       "      <td>[[job, title, occupational, therapist, area, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>202406</td>\n",
       "      <td>Hitachi Data Systems</td>\n",
       "      <td>Title: Pentaho-Sales Executive-Texas Location:...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Title, :, Pentaho-Sales, Executive-Texas, Lo...</td>\n",
       "      <td>[[title, location, texas, the, regional, accou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>111597</td>\n",
       "      <td>Franciscan St. Eilzabeth Health</td>\n",
       "      <td>Title: Respiratory Therapy, Intern Location: X...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Title, :, Respiratory, Therapy, ,, Intern, L...</td>\n",
       "      <td>[[title, respiratory, therapy, intern, location]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>260448</td>\n",
       "      <td>Intel</td>\n",
       "      <td>The Influencer Sales Group Solution Architect ...</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>45.52289</td>\n",
       "      <td>-122.98983</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[The, Influencer, Sales, Group, Solution, Arc...</td>\n",
       "      <td>[[the, influencer, sales, group, solution, arc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>197429</td>\n",
       "      <td>Hill-Rom</td>\n",
       "      <td>Title: Contract Svc Admin Location: United Sta...</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>42.78920</td>\n",
       "      <td>-85.51669</td>\n",
       "      <td>NaN</td>\n",
       "      <td>_ Other duties may be assigned:_</td>\n",
       "      <td>[[Title, :, Contract, Svc, Admin, Location, :,...</td>\n",
       "      <td>[[title, contract, svc, admin, location, unite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>359233</td>\n",
       "      <td>LHC Group</td>\n",
       "      <td>Job Description The Occupational Therapist is ...</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>41.51337</td>\n",
       "      <td>-87.67421</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Job, Description, The, Occupational, Therapi...</td>\n",
       "      <td>[[job, description, the, occupational, therapi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>5520</td>\n",
       "      <td>DCS Corporation</td>\n",
       "      <td>Systems Engineer - SE2-3678 Location: AL, Hunt...</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>34.73037</td>\n",
       "      <td>-86.58610</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Participate in preparation of technical and pr...</td>\n",
       "      <td>[[Systems, Engineer, -, SE2-3678, Location, :,...</td>\n",
       "      <td>[[systems, engineer, location, al, huntsville,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>205825</td>\n",
       "      <td>Home Depot</td>\n",
       "      <td>Position Purpose:Associates in Office/Store Su...</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>42.12509</td>\n",
       "      <td>-72.74954</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Usually in a comfortable environment but with ...</td>\n",
       "      <td>[[Position, Purpose, :, Associates, in, Office...</td>\n",
       "      <td>[[position, purpose, associates, in, support, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>68546</td>\n",
       "      <td>Education Corporation of America</td>\n",
       "      <td># Description Education Corporation of America...</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>33.52066</td>\n",
       "      <td>-86.80249</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Education Corporation of America owns and oper...</td>\n",
       "      <td>[[#, Description, Education, Corporation, of, ...</td>\n",
       "      <td>[[description, education, corporation, of, ame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>141067</td>\n",
       "      <td>Genesis Rehabilitation</td>\n",
       "      <td>Job Title: Physical Therapist Area of Interest...</td>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>42.99564</td>\n",
       "      <td>-71.45479</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Genesis Rehabilitation Services is looking for...</td>\n",
       "      <td>[[Job, Title, :, Physical, Therapist, Area, of...</td>\n",
       "      <td>[[job, title, physical, therapist, area, of, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>26925</td>\n",
       "      <td>Destination Hotels &amp; Resorts</td>\n",
       "      <td>Busser Property Paradise Point Resort &amp; Spa Co...</td>\n",
       "      <td>California</td>\n",
       "      <td>32.71533</td>\n",
       "      <td>-117.15726</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Busser, Property, Paradise, Point, Resort, &amp;...</td>\n",
       "      <td>[[busser, property, paradise, point, resort, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>81637</td>\n",
       "      <td>Epic Health Services</td>\n",
       "      <td>Epic Health Services, Inc. is looking for Bili...</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>39.95373</td>\n",
       "      <td>-74.19792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Provide skilled nursing care to pediatric and ...</td>\n",
       "      <td>[[Epic, Health, Services, ,, Inc., is, looking...</td>\n",
       "      <td>[[epic, health, services, is, looking, for, bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>191317</td>\n",
       "      <td>Hewlett Packard Enterprise Company</td>\n",
       "      <td>Title: Software Engineer Location: China-Shang...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Title, :, Software, Engineer, Location, :, C...</td>\n",
       "      <td>[[title, software, engineer, location, other, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>134395</td>\n",
       "      <td>Genesis Healthcare</td>\n",
       "      <td>Registered Nurse Area of Interest: Nursing - R...</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>42.57509</td>\n",
       "      <td>-70.93005</td>\n",
       "      <td>Registered Nurse / RN Requirements:;Bachelor's...</td>\n",
       "      <td>Twin Oaks in Danvers MA is looking for a Per D...</td>\n",
       "      <td>[[Registered, Nurse, Area, of, Interest, :, Nu...</td>\n",
       "      <td>[[registered, nurse, area, of, interest, nursi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>177803</td>\n",
       "      <td>Harris Corporation</td>\n",
       "      <td>Description: Job Title: Electrical Engineer &amp;n...</td>\n",
       "      <td>Florida</td>\n",
       "      <td>28.03446</td>\n",
       "      <td>-80.58866</td>\n",
       "      <td>Master&amp;rsquo s degree in electrical engineerin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Description, :, Job, Title, :, Electrical, E...</td>\n",
       "      <td>[[description, job, title, electrical, enginee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>222835</td>\n",
       "      <td>Home Depot</td>\n",
       "      <td>\"Position Purpose: Cashiers play a critical cu...</td>\n",
       "      <td>New Mexico</td>\n",
       "      <td>36.72806</td>\n",
       "      <td>-108.21869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Usually in a comfortable environment but with ...</td>\n",
       "      <td>[[``, Position, Purpose, :, Cashiers, play, a,...</td>\n",
       "      <td>[[position, purpose, cashiers, play, a, critic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>12790</td>\n",
       "      <td>Deloitte</td>\n",
       "      <td>Deloitte is one of the leading professional se...</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>39.95234</td>\n",
       "      <td>-75.16379</td>\n",
       "      <td>Establishes deliverable structure and content ...</td>\n",
       "      <td>Job Responsibilities;In providing finance-rela...</td>\n",
       "      <td>[[Deloitte, is, one, of, the, leading, profess...</td>\n",
       "      <td>[[deloitte, is, one, of, the, leading, profess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>319987</td>\n",
       "      <td>Kettering Medical Center</td>\n",
       "      <td>Description Greets patients and visitors in a ...</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>39.68950</td>\n",
       "      <td>-84.16883</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Description, Greets, patients, and, visitors...</td>\n",
       "      <td>[[description, greets, patients, and, visitors...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>352211</td>\n",
       "      <td>Learning Care Group</td>\n",
       "      <td># Job Description Join our talented team, wher...</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>33.42227</td>\n",
       "      <td>-111.82264</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Join our talented team, where we inspire child...</td>\n",
       "      <td>[[#, Job, Description, Join, our, talented, te...</td>\n",
       "      <td>[[job, description, join, our, talented, team,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>233278</td>\n",
       "      <td>Humana</td>\n",
       "      <td>Role: Technology Architect Assignment: IT Loca...</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>38.25424</td>\n",
       "      <td>-85.75941</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Guide all aspects of design, implementation an...</td>\n",
       "      <td>[[Role, :, Technology, Architect, Assignment, ...</td>\n",
       "      <td>[[role, technology, architect, assignment, it,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>338881</td>\n",
       "      <td>Kronos Incorporated</td>\n",
       "      <td>* Observes and reports activities and incident...</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>36.32311</td>\n",
       "      <td>-86.71333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[*, Observes, and, reports, activities, and, ...</td>\n",
       "      <td>[[observes, and, reports, activities, and, inc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>53773</td>\n",
       "      <td>Dominos Pizza</td>\n",
       "      <td>\"ABOUT THE JOB Do you know why Domino's Pizza ...</td>\n",
       "      <td>Texas</td>\n",
       "      <td>29.76328</td>\n",
       "      <td>-95.36327</td>\n",
       "      <td>General job duties for all store team members;...</td>\n",
       "      <td>\"You must be 18 years of age and have a valid ...</td>\n",
       "      <td>[[``, ABOUT, THE, JOB, Do, you, know, why, Dom...</td>\n",
       "      <td>[[about, the, job, do, you, know, why, domino,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>158203</td>\n",
       "      <td>Golden Living</td>\n",
       "      <td>Registered Dietician at AseraCare Hospice work...</td>\n",
       "      <td>Texas</td>\n",
       "      <td>29.76328</td>\n",
       "      <td>-95.36327</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Registered, Dietician, at, AseraCare, Hospic...</td>\n",
       "      <td>[[registered, dietician, at, aseracare, hospic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>156994</td>\n",
       "      <td>Golden Living</td>\n",
       "      <td>At Golden LivingCenters, we care for every pat...</td>\n",
       "      <td>South Dakota</td>\n",
       "      <td>44.08054</td>\n",
       "      <td>-103.23101</td>\n",
       "      <td>Currently licensed or registered in state of p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[At, Golden, LivingCenters, ,, we, care, for,...</td>\n",
       "      <td>[[at, golden, livingcenters, we, care, for, ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>249542</td>\n",
       "      <td>ICF International</td>\n",
       "      <td>Energy Efficiency and Low Income Representativ...</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>42.27756</td>\n",
       "      <td>-83.74088</td>\n",
       "      <td>/ /;/ /</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Energy, Efficiency, and, Low, Income, Repres...</td>\n",
       "      <td>[[energy, efficiency, and, low, income, repres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>173219</td>\n",
       "      <td>Hallmark Health</td>\n",
       "      <td>Office Coordinator Geriatric Assessment Center...</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>42.41843</td>\n",
       "      <td>-71.10616</td>\n",
       "      <td>NaN</td>\n",
       "      <td>High School Diploma or equivalent. Bachelor's ...</td>\n",
       "      <td>[[Office, Coordinator, Geriatric, Assessment, ...</td>\n",
       "      <td>[[office, coordinator, geriatric, assessment, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>50969</td>\n",
       "      <td>Dominos Pizza</td>\n",
       "      <td>Customer Service Representative Are you ready ...</td>\n",
       "      <td>South Dakota</td>\n",
       "      <td>42.87111</td>\n",
       "      <td>-97.39728</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Customer, Service, Representative, Are, you,...</td>\n",
       "      <td>[[customer, service, representative, are, you,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>331404</td>\n",
       "      <td>Kforce</td>\n",
       "      <td>Kforce has a client in Beaverton, Oregon (OR) ...</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>45.48706</td>\n",
       "      <td>-122.80371</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Kforce, has, a, client, in, Beaverton, ,, Or...</td>\n",
       "      <td>[[kforce, has, a, client, in, beaverton, orego...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>322250</td>\n",
       "      <td>Kforce</td>\n",
       "      <td>Kforce has a client in Stamford, Connecticut (...</td>\n",
       "      <td>Connecticut</td>\n",
       "      <td>41.05343</td>\n",
       "      <td>-73.53873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Kforce, has, a, client, in, Stamford, ,, Con...</td>\n",
       "      <td>[[kforce, has, a, client, in, stamford, connec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>157791</td>\n",
       "      <td>Golden Living</td>\n",
       "      <td>Here at Golden LivingCenters, we rely on and t...</td>\n",
       "      <td>Nebraska</td>\n",
       "      <td>40.67667</td>\n",
       "      <td>-95.85917</td>\n",
       "      <td>High school diploma or equivalent;Must within ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Here, at, Golden, LivingCenters, ,, we, rely...</td>\n",
       "      <td>[[here, at, golden, livingcenters, we, rely, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>119835</td>\n",
       "      <td>GameStop</td>\n",
       "      <td>\"*Description* Description: SUMMARY At GameSto...</td>\n",
       "      <td>New York</td>\n",
       "      <td>40.68149</td>\n",
       "      <td>-73.39984</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[``, *Description*, Description, :, SUMMARY, ...</td>\n",
       "      <td>[[description, summary, at, gamestop, we, refe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>285609</td>\n",
       "      <td>Jewel-Osco</td>\n",
       "      <td>\"Updated 6/2011 JOB TITLE: Service Clerk (Bagg...</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>41.66892</td>\n",
       "      <td>-87.73866</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Provides prompt, efficient and friendly custom...</td>\n",
       "      <td>[[``, Updated, 6/2011, JOB, TITLE, :, Service,...</td>\n",
       "      <td>[[updated, job, title, service, clerk, bagger,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>89260</td>\n",
       "      <td>Express Scripts</td>\n",
       "      <td>\"Schedule: Full-time Job ID: 1500071I The Sale...</td>\n",
       "      <td>Missouri</td>\n",
       "      <td>38.62727</td>\n",
       "      <td>-90.19789</td>\n",
       "      <td>NaN</td>\n",
       "      <td>o Proactive management of Houston entry &amp; comp...</td>\n",
       "      <td>[[``, Schedule, :, Full-time, Job, ID, :, 1500...</td>\n",
       "      <td>[[schedule, job, id, the, sales, coordinator, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>280096</td>\n",
       "      <td>JCPenney</td>\n",
       "      <td>Temp Support Specialist- Mall of Louisiana Loc...</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>30.45075</td>\n",
       "      <td>-91.15455</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Executes the merchandise strategy You take the...</td>\n",
       "      <td>[[Temp, Support, Specialist-, Mall, of, Louisi...</td>\n",
       "      <td>[[temp, support, mall, of, louisiana, location...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0 hiringOrganization_organizationName  \\\n",
       "0       158844       Golfsmith International, Inc.   \n",
       "1       257645                               Intel   \n",
       "2       107875                    Florida Hospital   \n",
       "3       202394                Hitachi Data Systems   \n",
       "4       109675           Footprint Retail Services   \n",
       "5       215973                          Home Depot   \n",
       "6       207524                          Home Depot   \n",
       "7        64426                      East West Bank   \n",
       "8       245192                                 IBM   \n",
       "9       202429                Hitachi Data Systems   \n",
       "10      269503             J&J Family of Companies   \n",
       "11      139164                  Genesis Healthcare   \n",
       "12      255915                      Ingersoll Rand   \n",
       "13      173294         HamiltonConstructionCompany   \n",
       "14      116855                      G6 Hospitality   \n",
       "15       40701                         Dollar Tree   \n",
       "16      292406              Johns Hopkins Medicine   \n",
       "17      358904                           LHC Group   \n",
       "18      126507                                  GE   \n",
       "19       90538                                  EY   \n",
       "20       59256             Dr Pepper Snapple Group   \n",
       "21       85716     Eurofins Lancaster Laboratories   \n",
       "22      203342                             HMSHOST   \n",
       "23      283327                          Jewel-Osco   \n",
       "24      320876       KeyPoint Government Solutions   \n",
       "25      311069                      Kelly Services   \n",
       "26      141835              Genesis Rehabilitation   \n",
       "27      202406                Hitachi Data Systems   \n",
       "28      111597     Franciscan St. Eilzabeth Health   \n",
       "29      260448                               Intel   \n",
       "..         ...                                 ...   \n",
       "70      197429                            Hill-Rom   \n",
       "71      359233                           LHC Group   \n",
       "72        5520                     DCS Corporation   \n",
       "73      205825                          Home Depot   \n",
       "74       68546    Education Corporation of America   \n",
       "75      141067              Genesis Rehabilitation   \n",
       "76       26925        Destination Hotels & Resorts   \n",
       "77       81637                Epic Health Services   \n",
       "78      191317  Hewlett Packard Enterprise Company   \n",
       "79      134395                  Genesis Healthcare   \n",
       "80      177803                  Harris Corporation   \n",
       "81      222835                          Home Depot   \n",
       "82       12790                            Deloitte   \n",
       "83      319987            Kettering Medical Center   \n",
       "84      352211                 Learning Care Group   \n",
       "85      233278                              Humana   \n",
       "86      338881                 Kronos Incorporated   \n",
       "87       53773                       Dominos Pizza   \n",
       "88      158203                       Golden Living   \n",
       "89      156994                       Golden Living   \n",
       "90      249542                   ICF International   \n",
       "91      173219                     Hallmark Health   \n",
       "92       50969                       Dominos Pizza   \n",
       "93      331404                              Kforce   \n",
       "94      322250                              Kforce   \n",
       "95      157791                       Golden Living   \n",
       "96      119835                            GameStop   \n",
       "97      285609                          Jewel-Osco   \n",
       "98       89260                     Express Scripts   \n",
       "99      280096                            JCPenney   \n",
       "\n",
       "                                       jobDescription  \\\n",
       "0   \"Sales Associate Tracking Code 220425-971 Job ...   \n",
       "1   For PHY system engineering team within the Wir...   \n",
       "2   *RN Medical Oncology PCU Orlando - Nights* Flo...   \n",
       "3   Title: Specialist Sales Account Representative...   \n",
       "4   **Footprint Retail Services** **Job Descriptio...   \n",
       "5   Position Purpose: Provide outstanding service ...   \n",
       "6   The Asset Protection Specialist is primarily r...   \n",
       "7   # Job Description East West Bank is one of the...   \n",
       "8   Job Description IBM is seeking to hire a Senio...   \n",
       "9   Title: Field Solutions Engineer Location: New ...   \n",
       "10  Project Manager (m/w) - Government & Public Af...   \n",
       "11  Certified Medicine Aide Area of Interest: Nurs...   \n",
       "12  **Description:** At Ingersoll Rand we're passi...   \n",
       "13  \"Hamilton Construction,in businesssince 1939 a...   \n",
       "14  Business Unit: DirectEmployers Title: Manager ...   \n",
       "15  Auto req ID 41872BR Title ASSISTANT MANAGER Em...   \n",
       "16  Johns Hopkins employs more than 20,000 people ...   \n",
       "17  Job Description As a CNA, you will perform a v...   \n",
       "18  2306307 **Business** GE Capital **Business Seg...   \n",
       "19  Title: Greater China TAS Referrals (China Sout...   \n",
       "20  Merchandiser The Merchandiser is responsible f...   \n",
       "21  ### Eurofins is the world leader in the food, ...   \n",
       "22  \"**Having the right ingredients makes all the ...   \n",
       "23  JOB: Retail Clerk  Liquor/Beer/Wine JOB OVERVI...   \n",
       "24  **Overview:** KeyPoint Government Solutions is...   \n",
       "25  **Strategic Sales Lead  Premier Brands and Con...   \n",
       "26  Job Title: Occupational Therapist Area of Inte...   \n",
       "27  Title: Pentaho-Sales Executive-Texas Location:...   \n",
       "28  Title: Respiratory Therapy, Intern Location: X...   \n",
       "29  The Influencer Sales Group Solution Architect ...   \n",
       "..                                                ...   \n",
       "70  Title: Contract Svc Admin Location: United Sta...   \n",
       "71  Job Description The Occupational Therapist is ...   \n",
       "72  Systems Engineer - SE2-3678 Location: AL, Hunt...   \n",
       "73  Position Purpose:Associates in Office/Store Su...   \n",
       "74  # Description Education Corporation of America...   \n",
       "75  Job Title: Physical Therapist Area of Interest...   \n",
       "76  Busser Property Paradise Point Resort & Spa Co...   \n",
       "77  Epic Health Services, Inc. is looking for Bili...   \n",
       "78  Title: Software Engineer Location: China-Shang...   \n",
       "79  Registered Nurse Area of Interest: Nursing - R...   \n",
       "80  Description: Job Title: Electrical Engineer &n...   \n",
       "81  \"Position Purpose: Cashiers play a critical cu...   \n",
       "82  Deloitte is one of the leading professional se...   \n",
       "83  Description Greets patients and visitors in a ...   \n",
       "84  # Job Description Join our talented team, wher...   \n",
       "85  Role: Technology Architect Assignment: IT Loca...   \n",
       "86  * Observes and reports activities and incident...   \n",
       "87  \"ABOUT THE JOB Do you know why Domino's Pizza ...   \n",
       "88  Registered Dietician at AseraCare Hospice work...   \n",
       "89  At Golden LivingCenters, we care for every pat...   \n",
       "90  Energy Efficiency and Low Income Representativ...   \n",
       "91  Office Coordinator Geriatric Assessment Center...   \n",
       "92  Customer Service Representative Are you ready ...   \n",
       "93  Kforce has a client in Beaverton, Oregon (OR) ...   \n",
       "94  Kforce has a client in Stamford, Connecticut (...   \n",
       "95  Here at Golden LivingCenters, we rely on and t...   \n",
       "96  \"*Description* Description: SUMMARY At GameSto...   \n",
       "97  \"Updated 6/2011 JOB TITLE: Service Clerk (Bagg...   \n",
       "98  \"Schedule: Full-time Job ID: 1500071I The Sale...   \n",
       "99  Temp Support Specialist- Mall of Louisiana Loc...   \n",
       "\n",
       "   jobLocation_address_region  jobLocation_geo_latitude  \\\n",
       "0                  California                  33.91918   \n",
       "1                         NaN                       NaN   \n",
       "2                     Florida                  28.53834   \n",
       "3                         NaN                       NaN   \n",
       "4                         NaN                       NaN   \n",
       "5                     Indiana                  41.13060   \n",
       "6                  New Jersey                  40.21455   \n",
       "7                  California                  34.06862   \n",
       "8                         NaN                       NaN   \n",
       "9                         NaN                       NaN   \n",
       "10                        NaN                       NaN   \n",
       "11                     Kansas                  39.10972   \n",
       "12                  Tennessee                  35.14953   \n",
       "13                     Oregon                  44.04624   \n",
       "14                      Texas                  32.83707   \n",
       "15                      Texas                  32.85791   \n",
       "16                   Maryland                  39.29038   \n",
       "17                    Alabama                  34.74481   \n",
       "18                        NaN                       NaN   \n",
       "19                        NaN                       NaN   \n",
       "20                      Texas                  29.76328   \n",
       "21                       Ohio                  39.55895   \n",
       "22                        NaN                       NaN   \n",
       "23                    Montana                  48.19579   \n",
       "24                        NaN                       NaN   \n",
       "25                        NaN                       NaN   \n",
       "26                        NaN                       NaN   \n",
       "27                        NaN                       NaN   \n",
       "28                        NaN                       NaN   \n",
       "29                     Oregon                  45.52289   \n",
       "..                        ...                       ...   \n",
       "70                   Michigan                  42.78920   \n",
       "71                   Illinois                  41.51337   \n",
       "72                    Alabama                  34.73037   \n",
       "73              Massachusetts                  42.12509   \n",
       "74                    Alabama                  33.52066   \n",
       "75              New Hampshire                  42.99564   \n",
       "76                 California                  32.71533   \n",
       "77                 New Jersey                  39.95373   \n",
       "78                        NaN                       NaN   \n",
       "79              Massachusetts                  42.57509   \n",
       "80                    Florida                  28.03446   \n",
       "81                 New Mexico                  36.72806   \n",
       "82               Pennsylvania                  39.95234   \n",
       "83                       Ohio                  39.68950   \n",
       "84                    Arizona                  33.42227   \n",
       "85                   Kentucky                  38.25424   \n",
       "86                  Tennessee                  36.32311   \n",
       "87                      Texas                  29.76328   \n",
       "88                      Texas                  29.76328   \n",
       "89               South Dakota                  44.08054   \n",
       "90                   Michigan                  42.27756   \n",
       "91              Massachusetts                  42.41843   \n",
       "92               South Dakota                  42.87111   \n",
       "93                     Oregon                  45.48706   \n",
       "94                Connecticut                  41.05343   \n",
       "95                   Nebraska                  40.67667   \n",
       "96                   New York                  40.68149   \n",
       "97                   Illinois                  41.66892   \n",
       "98                   Missouri                  38.62727   \n",
       "99                  Louisiana                  30.45075   \n",
       "\n",
       "    jobLocation_geo_longitude  \\\n",
       "0                  -118.41647   \n",
       "1                         NaN   \n",
       "2                   -81.37924   \n",
       "3                         NaN   \n",
       "4                         NaN   \n",
       "5                   -85.12886   \n",
       "6                   -74.61932   \n",
       "7                  -118.02757   \n",
       "8                         NaN   \n",
       "9                         NaN   \n",
       "10                        NaN   \n",
       "11                  -95.08775   \n",
       "12                  -90.04898   \n",
       "13                 -123.02203   \n",
       "14                  -97.08195   \n",
       "15                  -97.25474   \n",
       "16                  -76.61219   \n",
       "17                  -87.66753   \n",
       "18                        NaN   \n",
       "19                        NaN   \n",
       "20                  -95.36327   \n",
       "21                  -84.30411   \n",
       "22                        NaN   \n",
       "23                 -114.31291   \n",
       "24                        NaN   \n",
       "25                        NaN   \n",
       "26                        NaN   \n",
       "27                        NaN   \n",
       "28                        NaN   \n",
       "29                 -122.98983   \n",
       "..                        ...   \n",
       "70                  -85.51669   \n",
       "71                  -87.67421   \n",
       "72                  -86.58610   \n",
       "73                  -72.74954   \n",
       "74                  -86.80249   \n",
       "75                  -71.45479   \n",
       "76                 -117.15726   \n",
       "77                  -74.19792   \n",
       "78                        NaN   \n",
       "79                  -70.93005   \n",
       "80                  -80.58866   \n",
       "81                 -108.21869   \n",
       "82                  -75.16379   \n",
       "83                  -84.16883   \n",
       "84                 -111.82264   \n",
       "85                  -85.75941   \n",
       "86                  -86.71333   \n",
       "87                  -95.36327   \n",
       "88                  -95.36327   \n",
       "89                 -103.23101   \n",
       "90                  -83.74088   \n",
       "91                  -71.10616   \n",
       "92                  -97.39728   \n",
       "93                 -122.80371   \n",
       "94                  -73.53873   \n",
       "95                  -95.85917   \n",
       "96                  -73.39984   \n",
       "97                  -87.73866   \n",
       "98                  -90.19789   \n",
       "99                  -91.15455   \n",
       "\n",
       "                                       qualifications  \\\n",
       "0                                                 NaN   \n",
       "1                                                 NaN   \n",
       "2                                                 NaN   \n",
       "3                                                 NaN   \n",
       "4                                                 NaN   \n",
       "5                                                 NaN   \n",
       "6   Must be eighteen years of age or older. Must p...   \n",
       "7                                                 NaN   \n",
       "8                                                 NaN   \n",
       "9                                                 NaN   \n",
       "10                                                NaN   \n",
       "11                                                NaN   \n",
       "12  Considerable knowledge of industry related sup...   \n",
       "13                                                NaN   \n",
       "14  High School diploma or equivalent;Computer pro...   \n",
       "15                                                NaN   \n",
       "16                                                NaN   \n",
       "17                                                NaN   \n",
       "18  Doswiadczenie w pracy na podobnym stanowisku;W...   \n",
       "19                                                NaN   \n",
       "20                                                NaN   \n",
       "21                                                NaN   \n",
       "22                                                NaN   \n",
       "23                                                NaN   \n",
       "24                                                NaN   \n",
       "25  Bachelors degree (MBA preferred) or equivalent...   \n",
       "26                                                NaN   \n",
       "27                                                NaN   \n",
       "28                                                NaN   \n",
       "29                                                NaN   \n",
       "..                                                ...   \n",
       "70                                                NaN   \n",
       "71                                                NaN   \n",
       "72                                                NaN   \n",
       "73                                                NaN   \n",
       "74                                                NaN   \n",
       "75                                                NaN   \n",
       "76                                                NaN   \n",
       "77                                                NaN   \n",
       "78                                                NaN   \n",
       "79  Registered Nurse / RN Requirements:;Bachelor's...   \n",
       "80  Master&rsquo s degree in electrical engineerin...   \n",
       "81                                                NaN   \n",
       "82  Establishes deliverable structure and content ...   \n",
       "83                                                NaN   \n",
       "84                                                NaN   \n",
       "85                                                NaN   \n",
       "86                                                NaN   \n",
       "87  General job duties for all store team members;...   \n",
       "88                                                NaN   \n",
       "89  Currently licensed or registered in state of p...   \n",
       "90                                            / /;/ /   \n",
       "91                                                NaN   \n",
       "92                                                NaN   \n",
       "93                                                NaN   \n",
       "94                                                NaN   \n",
       "95  High school diploma or equivalent;Must within ...   \n",
       "96                                                NaN   \n",
       "97                                                NaN   \n",
       "98                                                NaN   \n",
       "99                                                NaN   \n",
       "\n",
       "                                     responsibilities  \\\n",
       "0   \"Ensure each Customer receives exceptional ser...   \n",
       "1                                                 NaN   \n",
       "2                                                 NaN   \n",
       "3                                                 NaN   \n",
       "4   A Merchandiser must complete all assigned merc...   \n",
       "5   Provide outstanding service to ensure efficien...   \n",
       "6                                                 NaN   \n",
       "7   We are currently seeking a Customer Service Ce...   \n",
       "8                                                 NaN   \n",
       "9   Job Functions;Specific duties in this role wil...   \n",
       "10                                                NaN   \n",
       "11  Receives report at the beginning of shift with...   \n",
       "12  Ingersoll Rand is a diverse and inclusive envi...   \n",
       "13                                                NaN   \n",
       "14                                                NaN   \n",
       "15                                                NaN   \n",
       "16                                                NaN   \n",
       "17  Your specific duties for this role will includ...   \n",
       "18  Prowadzenie projektow zw\\. z rozwojem pracowni...   \n",
       "19                                                NaN   \n",
       "20                                                NaN   \n",
       "21                                                NaN   \n",
       "22  \"Greets customers and takes food order provide...   \n",
       "23  Sells, receives, stocks and may order beer/liq...   \n",
       "24                                                NaN   \n",
       "25  This is a field based corporate sales position...   \n",
       "26  Screens, examines and evaluates patients, incl...   \n",
       "27                                                NaN   \n",
       "28                                                NaN   \n",
       "29                                                NaN   \n",
       "..                                                ...   \n",
       "70                   _ Other duties may be assigned:_   \n",
       "71                                                NaN   \n",
       "72  Participate in preparation of technical and pr...   \n",
       "73  Usually in a comfortable environment but with ...   \n",
       "74  Education Corporation of America owns and oper...   \n",
       "75  Genesis Rehabilitation Services is looking for...   \n",
       "76                                                NaN   \n",
       "77  Provide skilled nursing care to pediatric and ...   \n",
       "78                                                NaN   \n",
       "79  Twin Oaks in Danvers MA is looking for a Per D...   \n",
       "80                                                NaN   \n",
       "81  Usually in a comfortable environment but with ...   \n",
       "82  Job Responsibilities;In providing finance-rela...   \n",
       "83                                                NaN   \n",
       "84  Join our talented team, where we inspire child...   \n",
       "85  Guide all aspects of design, implementation an...   \n",
       "86                                                NaN   \n",
       "87  \"You must be 18 years of age and have a valid ...   \n",
       "88                                                NaN   \n",
       "89                                                NaN   \n",
       "90                                                NaN   \n",
       "91  High School Diploma or equivalent. Bachelor's ...   \n",
       "92                                                NaN   \n",
       "93                                                NaN   \n",
       "94                                                NaN   \n",
       "95                                                NaN   \n",
       "96                                                NaN   \n",
       "97  Provides prompt, efficient and friendly custom...   \n",
       "98  o Proactive management of Houston entry & comp...   \n",
       "99  Executes the merchandise strategy You take the...   \n",
       "\n",
       "                                      tokenized_sents  \\\n",
       "0   [[``, Sales, Associate, Tracking, Code, 220425...   \n",
       "1   [[For, PHY, system, engineering, team, within,...   \n",
       "2   [[*RN, Medical, Oncology, PCU, Orlando, -, Nig...   \n",
       "3   [[Title, :, Specialist, Sales, Account, Repres...   \n",
       "4   [[**Footprint, Retail, Services**, **Job, Desc...   \n",
       "5   [[Position, Purpose, :, Provide, outstanding, ...   \n",
       "6   [[The, Asset, Protection, Specialist, is, prim...   \n",
       "7   [[#, Job, Description, East, West, Bank, is, o...   \n",
       "8   [[Job, Description, IBM, is, seeking, to, hire...   \n",
       "9   [[Title, :, Field, Solutions, Engineer, Locati...   \n",
       "10  [[Project, Manager, (, m/w, ), -, Government, ...   \n",
       "11  [[Certified, Medicine, Aide, Area, of, Interes...   \n",
       "12  [[**Description, :, **, At, Ingersoll, Rand, w...   \n",
       "13  [[``, Hamilton, Construction, ,, in, businesss...   \n",
       "14  [[Business, Unit, :, DirectEmployers, Title, :...   \n",
       "15  [[Auto, req, ID, 41872BR, Title, ASSISTANT, MA...   \n",
       "16  [[Johns, Hopkins, employs, more, than, 20,000,...   \n",
       "17  [[Job, Description, As, a, CNA, ,, you, will, ...   \n",
       "18  [[2306307, **Business**, GE, Capital, **Busine...   \n",
       "19  [[Title, :, Greater, China, TAS, Referrals, (,...   \n",
       "20  [[Merchandiser, The, Merchandiser, is, respons...   \n",
       "21  [[#, #, #, Eurofins, is, the, world, leader, i...   \n",
       "22  [[``, **Having, the, right, ingredients, makes...   \n",
       "23  [[JOB, :, Retail, Clerk, Liquor/Beer/Wine, JOB...   \n",
       "24  [[**Overview, :, **, KeyPoint, Government, Sol...   \n",
       "25  [[**Strategic, Sales, Lead, Premier, Brands, a...   \n",
       "26  [[Job, Title, :, Occupational, Therapist, Area...   \n",
       "27  [[Title, :, Pentaho-Sales, Executive-Texas, Lo...   \n",
       "28  [[Title, :, Respiratory, Therapy, ,, Intern, L...   \n",
       "29  [[The, Influencer, Sales, Group, Solution, Arc...   \n",
       "..                                                ...   \n",
       "70  [[Title, :, Contract, Svc, Admin, Location, :,...   \n",
       "71  [[Job, Description, The, Occupational, Therapi...   \n",
       "72  [[Systems, Engineer, -, SE2-3678, Location, :,...   \n",
       "73  [[Position, Purpose, :, Associates, in, Office...   \n",
       "74  [[#, Description, Education, Corporation, of, ...   \n",
       "75  [[Job, Title, :, Physical, Therapist, Area, of...   \n",
       "76  [[Busser, Property, Paradise, Point, Resort, &...   \n",
       "77  [[Epic, Health, Services, ,, Inc., is, looking...   \n",
       "78  [[Title, :, Software, Engineer, Location, :, C...   \n",
       "79  [[Registered, Nurse, Area, of, Interest, :, Nu...   \n",
       "80  [[Description, :, Job, Title, :, Electrical, E...   \n",
       "81  [[``, Position, Purpose, :, Cashiers, play, a,...   \n",
       "82  [[Deloitte, is, one, of, the, leading, profess...   \n",
       "83  [[Description, Greets, patients, and, visitors...   \n",
       "84  [[#, Job, Description, Join, our, talented, te...   \n",
       "85  [[Role, :, Technology, Architect, Assignment, ...   \n",
       "86  [[*, Observes, and, reports, activities, and, ...   \n",
       "87  [[``, ABOUT, THE, JOB, Do, you, know, why, Dom...   \n",
       "88  [[Registered, Dietician, at, AseraCare, Hospic...   \n",
       "89  [[At, Golden, LivingCenters, ,, we, care, for,...   \n",
       "90  [[Energy, Efficiency, and, Low, Income, Repres...   \n",
       "91  [[Office, Coordinator, Geriatric, Assessment, ...   \n",
       "92  [[Customer, Service, Representative, Are, you,...   \n",
       "93  [[Kforce, has, a, client, in, Beaverton, ,, Or...   \n",
       "94  [[Kforce, has, a, client, in, Stamford, ,, Con...   \n",
       "95  [[Here, at, Golden, LivingCenters, ,, we, rely...   \n",
       "96  [[``, *Description*, Description, :, SUMMARY, ...   \n",
       "97  [[``, Updated, 6/2011, JOB, TITLE, :, Service,...   \n",
       "98  [[``, Schedule, :, Full-time, Job, ID, :, 1500...   \n",
       "99  [[Temp, Support, Specialist-, Mall, of, Louisi...   \n",
       "\n",
       "                                     normalized_sents  \n",
       "0   [[sales, associate, tracking, code, job, descr...  \n",
       "1   [[for, phy, system, engineering, team, within,...  \n",
       "2   [[medical, oncology, pcu, orlando, florida, ho...  \n",
       "3   [[title, specialist, sales, account, represent...  \n",
       "4   [[retail, job, title, retail, merchandiser, re...  \n",
       "5   [[position, purpose, provide, outstanding, ser...  \n",
       "6   [[the, asset, protection, specialist, is, prim...  \n",
       "7   [[job, description, east, west, bank, is, one,...  \n",
       "8   [[job, description, ibm, is, seeking, to, hire...  \n",
       "9   [[title, field, solutions, engineer, location,...  \n",
       "10  [[project, manager, government, public, jansse...  \n",
       "11  [[certified, medicine, aide, area, of, interes...  \n",
       "12  [[at, ingersoll, rand, we, passionate, about, ...  \n",
       "13  [[hamilton, construction, in, businesssince, a...  \n",
       "14  [[business, unit, directemployers, title, mana...  \n",
       "15  [[auto, req, id, title, assistant, manager, em...  \n",
       "16  [[johns, hopkins, employs, more, than, people,...  \n",
       "17  [[job, description, as, a, cna, you, will, per...  \n",
       "18  [[ge, capital, capital, international, bank, b...  \n",
       "19  [[title, greater, china, tas, referrals, china...  \n",
       "20  [[merchandiser, the, merchandiser, is, respons...  \n",
       "21  [[eurofins, is, the, world, leader, in, the, f...  \n",
       "22  [[the, right, ingredients, makes, all, the, di...  \n",
       "23  [[job, retail, clerk, job, overview, provides,...  \n",
       "24  [[keypoint, government, solutions, is, current...  \n",
       "25  [[sales, lead, premier, brands, and, consumer,...  \n",
       "26  [[job, title, occupational, therapist, area, o...  \n",
       "27  [[title, location, texas, the, regional, accou...  \n",
       "28  [[title, respiratory, therapy, intern, location]]  \n",
       "29  [[the, influencer, sales, group, solution, arc...  \n",
       "..                                                ...  \n",
       "70  [[title, contract, svc, admin, location, unite...  \n",
       "71  [[job, description, the, occupational, therapi...  \n",
       "72  [[systems, engineer, location, al, huntsville,...  \n",
       "73  [[position, purpose, associates, in, support, ...  \n",
       "74  [[description, education, corporation, of, ame...  \n",
       "75  [[job, title, physical, therapist, area, of, i...  \n",
       "76  [[busser, property, paradise, point, resort, s...  \n",
       "77  [[epic, health, services, is, looking, for, bi...  \n",
       "78  [[title, software, engineer, location, other, ...  \n",
       "79  [[registered, nurse, area, of, interest, nursi...  \n",
       "80  [[description, job, title, electrical, enginee...  \n",
       "81  [[position, purpose, cashiers, play, a, critic...  \n",
       "82  [[deloitte, is, one, of, the, leading, profess...  \n",
       "83  [[description, greets, patients, and, visitors...  \n",
       "84  [[job, description, join, our, talented, team,...  \n",
       "85  [[role, technology, architect, assignment, it,...  \n",
       "86  [[observes, and, reports, activities, and, inc...  \n",
       "87  [[about, the, job, do, you, know, why, domino,...  \n",
       "88  [[registered, dietician, at, aseracare, hospic...  \n",
       "89  [[at, golden, livingcenters, we, care, for, ev...  \n",
       "90  [[energy, efficiency, and, low, income, repres...  \n",
       "91  [[office, coordinator, geriatric, assessment, ...  \n",
       "92  [[customer, service, representative, are, you,...  \n",
       "93  [[kforce, has, a, client, in, beaverton, orego...  \n",
       "94  [[kforce, has, a, client, in, stamford, connec...  \n",
       "95  [[here, at, golden, livingcenters, we, rely, o...  \n",
       "96  [[description, summary, at, gamestop, we, refe...  \n",
       "97  [[updated, job, title, service, clerk, bagger,...  \n",
       "98  [[schedule, job, id, the, sales, coordinator, ...  \n",
       "99  [[temp, support, mall, of, louisiana, location...  \n",
       "\n",
       "[100 rows x 10 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleDF = pandas.DataFrame.from_csv('data/SampleJobAds.csv', index_col = False)\n",
    "#We need to convert the last couple columns from strings to lists\n",
    "sampleDF['tokenized_sents'] = sampleDF['tokenized_sents'].apply(lambda x: eval(x))\n",
    "sampleDF['normalized_sents'] = sampleDF['normalized_sents'].apply(lambda x: eval(x))\n",
    "sampleDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's define a function to calculate the likelihood of each job description. The idea is borrowed from [Matt Taddy](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/deepir.ipynb), who shows how a document can be characterized as the inner product of the distance between its words. In other words, this analysis will show which job ads are most likely to find an appropriate pool of workers in the resume bank that generated our word embedding.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adprob(ad, model):\n",
    "    sen_scores = model.score(ad, len(ad))\n",
    "    ad_score = sen_scores.mean()\n",
    "    return ad_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply this function to every job description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "we have only implemented score for hs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-942c0b2dd6ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msampleDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'likelihood'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampleDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'normalized_sents'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0madprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   2167\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimestamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2169\u001b[0;31m         \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2171\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/src/inference.pyx\u001b[0m in \u001b[0;36mpandas.lib.map_infer (pandas/lib.c:62578)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-942c0b2dd6ae>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msampleDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'likelihood'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampleDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'normalized_sents'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0madprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-6782f974d8d7>\u001b[0m in \u001b[0;36madprob\u001b[0;34m(ad, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0madprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msen_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mad_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msen_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mad_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, sentences, total_sentences, chunksize, queue_factor, report_delay)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"we have only implemented score for hs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mworker_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: we have only implemented score for hs"
     ]
    }
   ],
   "source": [
    "sampleDF['likelihood'] = sampleDF['normalized_sents'].apply(lambda x: adprob(x, resume_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the top 5 job descriptions that have the highest likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ad in sampleDF.sort_values(by = 'likelihood', ascending = False)['jobDescription'][:5]:\n",
    "    print (ad + '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the bottom 5 job descriptions that have the lowest likelihood to be matched by the resumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ad in sampleDF.sort_values(by = 'likelihood')['jobDescription'][:5]:\n",
    "    print (ad + '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same for phrases corresponding to job skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adprob([[\"python\", \"programming\"]], resume_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adprob([[\"basic\", \"programming\"]], resume_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic programming appears to be more likely in this pool of resumes than python programming. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do some simple statistics. Unfortunately, we don't have a large sample here. Nevertheless, let's first look at the mean likelihood score of each hiring organization. Some organizations will do well to hire on CareerBuilder...while others will not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampleDF.groupby(\"hiringOrganization_organizationName\")[['likelihood']].mean().sort_values('likelihood', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the mean likelihood of each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampleDF.groupby(\"jobLocation_address_region\")[['likelihood']].mean().sort_values('likelihood', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You would increase the sample size if you want to do a more serious study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Your Turn*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that calculate the scores for a small sample of documents from outside your corpus to identify which are *closest* to your corpus. Then calculate the scores for a few phrases or sentences to identify the ones most likely to have appeared in your corpus. Interrogate patterns associated with these document/phrase scores (e.g., which companies produced job ads most or least likely to find jobseekers in the resume corpus?) What do these patterns suggest about the boundaries of your corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also project word vectors to an arbitray semantic dimension. To demonstrate this possibility, let's first load a model trained with New York Times news articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nytimes_model = gensim.models.word2vec.Word2Vec.load_word2vec_format('/mnt/efs/resources/shared/Notebook-4-data/nytimes_cbow.reduced.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we can visualize with dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#words to create dimensions\n",
    "tnytTargetWords = ['man','him','he', 'woman', 'her', 'she', 'black','blacks','African', 'white', 'whites', 'Caucasian', 'rich', 'richer', 'richest', 'expensive', 'wealthy', 'poor', 'poorer', 'poorest', 'cheap', 'inexpensive']\n",
    "#words we will be mapping\n",
    "tnytTargetWords += [\"doctor\",\"lawyer\",\"plumber\",\"scientist\",\"hairdresser\", \"nanny\",\"carpenter\",\"entrepreneur\",\"musician\",\"writer\", \"banker\",\"poet\",\"nurse\", \"steak\", \"bacon\", \"croissant\", \"cheesecake\", \"salad\", \"cheeseburger\", \"vegetables\", \"beer\", \"wine\", \"pastry\", \"basketball\", \"baseball\", \"boxing\", \"softball\", \"volleyball\", \"tennis\", \"golf\", \"hockey\", \"soccer\"]\n",
    "\n",
    "\n",
    "wordsSubMatrix = []\n",
    "for word in tnytTargetWords:\n",
    "    wordsSubMatrix.append(nytimes_model[word])\n",
    "wordsSubMatrix = np.array(wordsSubMatrix)\n",
    "wordsSubMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pcaWordsNYT = sklearn.decomposition.PCA(n_components = 50).fit(wordsSubMatrix)\n",
    "reducedPCA_dataNYT = pcaWordsNYT.transform(wordsSubMatrix)\n",
    "#T-SNE is theoretically better, but you should experiment\n",
    "tsneWordsNYT = sklearn.manifold.TSNE(n_components = 2).fit_transform(reducedPCA_dataNYT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tsneWordsNYT[:,1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_frame_on(False)\n",
    "plt.scatter(tsneWordsNYT[:, 0], tsneWordsNYT[:, 1], alpha = 0) #Making the points invisible\n",
    "for i, word in enumerate(tnytTargetWords):\n",
    "    ax.annotate(word, (tsneWordsNYT[:, 0][i],tsneWordsNYT[:, 1][i]), size =  20 * (len(tnytTargetWords) - i) / len(tnytTargetWords))\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some convenient functions for getting dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(vector):\n",
    "    normalized_vector = vector / np.linalg.norm(vector)\n",
    "    return normalized_vector\n",
    "\n",
    "def dimension(model, positives, negatives):\n",
    "    diff = sum([normalize(model[x]) for x in positives]) - sum([normalize(model[y]) for y in negatives])\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#nytimes_model['already']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate three dimensions: gender, race, and class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Gender = dimension(nytimes_model, ['man','him','he'], ['woman', 'her', 'she'])\n",
    "Race = dimension(nytimes_model, ['black','blacks','African'], ['white', 'whites', 'Caucasian'])\n",
    "Class = dimension(nytimes_model, ['rich', 'richer', 'richest', 'expensive', 'wealthy'], ['poor', 'poorer', 'poorest', 'cheap', 'inexpensive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have some words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Occupations = [\"doctor\",\"lawyer\",\"plumber\",\"scientist\",\"hairdresser\", \"nanny\",\"carpenter\",\"entrepreneur\",\"musician\",\"writer\", \"banker\",\"poet\",\"nurse\"]\n",
    "\n",
    "Foods = [\"steak\", \"bacon\", \"croissant\", \"cheesecake\", \"salad\", \"cheeseburger\", \"vegetables\", \"beer\", \"wine\", \"pastry\"]\n",
    "\n",
    "Sports  = [\"basketball\", \"baseball\", \"boxing\", \"softball\", \"volleyball\", \"tennis\", \"golf\", \"hockey\", \"soccer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to project words in a word list to each of the three dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def makeDF(model, word_list):\n",
    "    g = []\n",
    "    r = []\n",
    "    c = []\n",
    "    for word in word_list:\n",
    "        g.append(sklearn.metrics.pairwise.cosine_similarity(nytimes_model[word].reshape(1,-1), Gender.reshape(1,-1))[0][0])\n",
    "        r.append(sklearn.metrics.pairwise.cosine_similarity(nytimes_model[word].reshape(1,-1), Race.reshape(1,-1))[0][0])\n",
    "        c.append(sklearn.metrics.pairwise.cosine_similarity(nytimes_model[word].reshape(1,-1), Class.reshape(1,-1))[0][0])\n",
    "    df = pandas.DataFrame({'gender': g, 'race': r, 'class': c}, index = word_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OCCdf = makeDF(nytimes_model, Occupations) \n",
    "Fooddf = makeDF(nytimes_model, Foods)\n",
    "Sportsdf = makeDF(nytimes_model, Sports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some useful functions for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Coloring(Series):\n",
    "    x = Series.values\n",
    "    y = x-x.min()\n",
    "    z = y/y.max()\n",
    "    c = list(plt.cm.rainbow(z))\n",
    "    return c\n",
    "\n",
    "def PlotDimension(ax,df, dim):\n",
    "    ax.set_frame_on(False)\n",
    "    ax.set_title(dim, fontsize = 20)\n",
    "    colors = Coloring(df[dim])\n",
    "    for i, word in enumerate(df.index):\n",
    "        ax.annotate(word, (0, df[dim][i]), color = colors[i], alpha = 0.6, fontsize = 12)\n",
    "    MaxY = df[dim].max()\n",
    "    MinY = df[dim].min()\n",
    "    plt.ylim(MinY,MaxY)\n",
    "    plt.yticks(())\n",
    "    plt.xticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the occupational words in each of the three dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12,4))\n",
    "ax1 = fig.add_subplot(131)\n",
    "PlotDimension(ax1, OCCdf, 'gender')\n",
    "ax2 = fig.add_subplot(132)\n",
    "PlotDimension(ax2, OCCdf, 'race')\n",
    "ax3 = fig.add_subplot(133)\n",
    "PlotDimension(ax3, OCCdf, 'class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12,4))\n",
    "ax1 = fig.add_subplot(131)\n",
    "PlotDimension(ax1, Fooddf, 'gender')\n",
    "ax2 = fig.add_subplot(132)\n",
    "PlotDimension(ax2, Fooddf, 'race')\n",
    "ax3 = fig.add_subplot(133)\n",
    "PlotDimension(ax3, Fooddf, 'class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12,4))\n",
    "ax1 = fig.add_subplot(131)\n",
    "PlotDimension(ax1, Sportsdf, 'gender')\n",
    "ax2 = fig.add_subplot(132)\n",
    "PlotDimension(ax2, Sportsdf, 'race')\n",
    "ax3 = fig.add_subplot(133)\n",
    "PlotDimension(ax3, Sportsdf, 'class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <span style=\"color:red\">*Your Turn*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that identify semantic dimensions of interest from your data (e.g., gender: man-woman) and project words onto these dimensions. Plot the array of relevant words along each semantic dimension. Which words are most different. Which dimensions are most different? On which dimension are your words most different? Print three short textual examples from the corpus that illustrate the association you have explored.\n",
    "\n",
    "<span style=\"color:red\">***Stretch***: Project documents from your corpus along a dimension of interest. Sample relevant documents from your corpus with this functionality and explain your rationale? Calculate the cosine of the angle between two dimensions (encoded as vectors) of interest. What does this suggest about the relationship between them within your corpus? \n",
    "\n",
    "<span style=\"color:red\">***Super stretch***: Create 90% bootstrap confidence intervals around your word projections onto a given dimension by generating 10 separate word2vec models, sampling $n$ documents (the total number in your corpus) for each, but with replacement. The bounds will be defined as the highest and lowest projection across your 10 samples. Which words are *significantly* different on your semantic dimension of interest?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
