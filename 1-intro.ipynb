{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layout\n",
    "\n",
    "+ opening\n",
    "    + contents\n",
    "    + files used\n",
    "    + packages used\n",
    "+ Scraping\n",
    "    + Images\n",
    "        + OCR\n",
    "    + raw text\n",
    "    + html\n",
    "    + PDFs\n",
    "        + Need stable url\n",
    "    + word doc etc.\n",
    "        + Need example word doc\n",
    "+ spidering\n",
    "    + wikipedia\n",
    "    + APIs\n",
    "        + REST\n",
    "        + tumblr\n",
    "            + Newly registered consumers are rate limited to 250 requests per hour, and 5,000 requests per day. If your application requires more requests for either of these periods, please use the 'Request rate limit removal' link on an app above.\n",
    "            + Reid McIlroy-Young OAuth\n",
    "                + Consumer Key: TgqpubaBeckUPRHWUCTHIe2DzGYyZ0hXYFenh2tiyZMGv874h8\n",
    "                + Secret Key:  GTXHKip2c8TJyMz9A2iRhrV1cx03FSaSaznXGoVvCW2Fx5lyCv\n",
    "+ reading files\n",
    "    + encodings\n",
    "    + unicode\n",
    "+ filtering\n",
    "+ data structures\n",
    "    + pandas\n",
    "\n",
    "\n",
    "# Week 1 - Intro\n",
    "\n",
    "Intro stuff ...\n",
    "\n",
    "For this notebook we will be using the following packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All these packages need to be installed from pip\n",
    "import requests #for http requests\n",
    "import bs4 #called 'BeautifulSoup', an html parser\n",
    "import pandas #gives us DataFrames\n",
    "import docx #reading MS doc files, install as `python-docx`\n",
    "\n",
    "#Stuff for pdfs\n",
    "import pdfminer.pdfinterp\n",
    "import pdfminer.converter\n",
    "import pdfminer.layout\n",
    "import pdfminer.pdfpage\n",
    "\n",
    "#These come with Python\n",
    "import re #for regexs\n",
    "import urllib.parse #For joining urls\n",
    "import io #for making http requests look like files\n",
    "import json #For Tumblr API responses\n",
    "import os.path #For checking if files exist\n",
    "import os #For making directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also be working on the following files/urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_base_url = 'https://en.wikipedia.org'\n",
    "wikipedia_content_analysis = 'https://en.wikipedia.org/wiki/Content_analysis'\n",
    "content_analysis_save = 'wikipedia_content_analysis.html'\n",
    "example_text_file = 'sometextfile.txt'\n",
    "information_extraction_pdf = 'https://web.stanford.edu/~jurafsky/slp3/21.pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping\n",
    "\n",
    "Before we can start analyzing content we need to obtain it. Sometimes it will be provided to us before hand, but often we will need to download it. As a starting example we will attempt to download the wikipedia page on content analysis. The page is located at [https://en.wikipedia.org/wiki/Content_analysis](https://en.wikipedia.org/wiki/Content_analysis) so lets start with that.\n",
    "\n",
    "We can do this by making an HTTP GET request to that url, a GET request is simply a request to the server to provide the contents given by some url. The other request we will be using in this class is called a POST request and requests the server to take some content we provide. While the Python standard library does have the ability do make GET requests we will be using the [_requests_](http://docs.python-requests.org/en/master/) package as it is _'the only Non-GMO HTTP library for Python'_, also it provides a nicer interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#wikipedia_content_analysis = 'https://en.wikipedia.org/wiki/Content_analysis'\n",
    "requests.get('https://en.wikipedia.org/wiki/Content_analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`'Response [200]'` means the server responded with what we asked for. If you get another number (e.g. 404) it likely means there was some kind of error, these codes are called HTTP response codes and a list of them can be found [here](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes). The response object contains all the data the server sent including the website's contents and the HTTP header. We are interested in the contents which we can access with the `.text` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\n",
      "<head>\n",
      "<meta charset=\"UTF-8\"/>\n",
      "<title>Content analysis - Wikipedia</title>\n",
      "<script>document.documentElement.className = document.documentElement.className.replace( /(^|\\s)client-nojs(\\s|$)/, \"$1client-js$2\" );</script>\n",
      "<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"Content_analysis\",\"wgTitle\":\"Content analysis\",\"wgCurRevisionId\":750367700,\"wgRevisionId\":750367700,\"wgArticleId\":473317,\"wgIsArticle\":true,\"wgIsRedirect\":false,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"Articles needing cleanup from April 2008\",\"All articles needing cleanup\",\"Cleanup tagged articles without a reason field from April 2008\",\"Wikipedia pages needing cleanup from April 2008\",\"Articles needing expert attention with no reason or talk parameter\",\"Articles needing expert attention from April 2008\",\"All artic\n"
     ]
    }
   ],
   "source": [
    "wikiContentRequest = requests.get('https://en.wikipedia.org/wiki/Content_analysis')\n",
    "print(wikiContentRequest.text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not what we were looking for, because it is the start of the HTML that makes up the website. This is HTML and is meant to be read by computers. Luckily we have a computer to parse it for us. To do the parsing we will use [_Beautiful Soup_](https://www.crummy.com/software/BeautifulSoup/) which is a better parser than the one in the standard library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Content analysis - Wikipedia\n",
      "document.documentElement.className = document.documentElement.className.replace( /(^|\\s)client-nojs(\\s|$)/, \"$1client-js$2\" );\n",
      "(window.RLQ=window.RLQ||[]).push(functio\n"
     ]
    }
   ],
   "source": [
    "wikiContentSoup = bs4.BeautifulSoup(wikiContentRequest.text, 'html.parser')\n",
    "print(wikiContentSoup.text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better but there's a bunch of random whitespace and we have way more than just the text of the article. This is because what we requested is the whole webpage, not just the text for the article.\n",
    "\n",
    "We need to extract only the text we care about, in order to do this we will need to inspect the html. One way to do this is to simply go to the website with a browser and use its inspection or view source tool, but if there is javascript or other dynamic loading occurring on the page it is very likely that what Python receives is not what you will see. So we will need to view what Python receives. To do this we can save the html `requests` obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#content_analysis_save = 'wikipedia_content_analysis.html'\n",
    "\n",
    "with open(content_analysis_save, mode='w', encoding='utf-8') as f:\n",
    "    f.write(wikiContentRequest.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets open the file (`wikipedia_content_analysis.html`) we just created with a web browser. It should look sort of like the original but missing all the images and formatting.\n",
    "\n",
    "As there is very little standardization on structuring webpages figuring out how best to extract what you want is an art. Looking at this page it looks like all the main textual content is within `<p>`(paragraph) tags inside the `<body>` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content analysis is \"a wide and heterogeneous set of manual or computer-assisted techniques for contextualized interpretations of documents produced by communication processes in the strict sense of that phrase (any kind of text, written, iconic, multimedia, etc.) or signification processes (traces and artifacts), having as ultimate goal the production of valid and trustworthy inferences.\"\n",
      "Content analysis has come to be a sort of 'umbrella term' referring to an almost boundless set of quite diverse research approaches and techniques. Broadly, it can refer to methods for studying and/or retrieving meaningful information from documents.[1] In a more focused way, content analysis refers to a family of techniques for studying the \"mute evidence\" of texts and artifacts.[2] There are 5 types of texts in content analysis:\n",
      "Content analysis can also be described as studying traces, which are documents from past times, and artifacts, which are non-linguistic documents. Texts are understood to be produced by communication processes in a broad sense of that phrase - often gaining mean through abduction.[1][3]\n"
     ]
    }
   ],
   "source": [
    "contentPTags = wikiContentSoup.body.findAll('p')\n",
    "for pTag in contentPTags[:3]:\n",
    "    print(pTag.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the text from the page, split up by paragraph. If we wanted to get the section headers or references as well it would require a bit more work, but is doable.\n",
    "\n",
    "There is one more thing we might want to do before sending this text to be processed, remove the references indicators (`[2]`, `[3]` , etc). To do this we can use a short regular expression (regex)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       paragraph-text\n",
      "0   Content analysis is \"a wide and heterogeneous ...\n",
      "1   Content analysis has come to be a sort of 'umb...\n",
      "2   Content analysis can also be described as stud...\n",
      "3   Despite the wide variety of options, generally...\n",
      "4   Over the years, content analysis has been appl...\n",
      "5   In recent times, particularly with the advent ...\n",
      "6   Quantitative content analysis has enjoyed a re...\n",
      "7                                                    \n",
      "8                                                    \n",
      "9   The method of content analysis enables the res...\n",
      "10  Since the 1980s, content analysis has become a...\n",
      "11  The creation of coding frames is intrinsically...\n",
      "12  Mimetic Convergence thus aims to show the proc...\n",
      "13  Every content analysis should depart from a hy...\n",
      "14  As an evaluation approach, content analysis is...\n",
      "15  Qualitative content analysis is “a systematic,...\n",
      "16  Holsti groups fifteen uses of content analysis...\n",
      "17  He also places these uses into the context of ...\n",
      "18  The following table shows fifteen uses of cont...\n",
      "19  According to Dr. Klaus Krippendorff, six quest...\n",
      "20  The assumption is that words and phrases menti...\n",
      "21  Qualitatively, content analysis can involve an...\n",
      "22  Normally, content analysis can only be applied...\n",
      "23  A further step in analysis is the distinction ...\n",
      "24  Dermot McKeone highlighted the difference betw...\n",
      "25  As the uncritical use of text is today widely ...\n",
      "26  Neuendorf suggests that when human coders are ...\n"
     ]
    }
   ],
   "source": [
    "contentParagraphs = []\n",
    "for pTag in contentPTags:\n",
    "    #strings starting with r are raw so their \\'s are not modifier characters\n",
    "    #If we didn't start with r the string would be: '\\\\[\\\\d+\\\\]'\n",
    "    contentParagraphs.append(re.sub(r'\\[\\d+\\]', '', pTag.text))\n",
    "\n",
    "#convert to a DataFrame\n",
    "contentParagraphsDF = pandas.DataFrame({'paragraph-text' : contentParagraphs})\n",
    "print(contentParagraphsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a `DataFrame` of all the relevant text from the page ready to be processed\n",
    "\n",
    "If you are not familiar with regex, it is a way of specifying searches of text. A regex engine takes in the search pattern, in the above case `'\\[\\d+\\]'` and some string, the paragraph texts. Then it reads the input string one character at a time checking if it matches the search. For example the regex `'\\d'` matches number characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(36, 37), match='2'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findNumber = r'\\d'\n",
    "regexResults = re.search(findNumber, 'not a number, not a number, numbers 2134567890, not a number')\n",
    "regexResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python the regex package (`re`) usually returns `Match` objects (you can have multiple pattern hits in a a single `Match`), to get the string that matched our pattern we can use the `.group()` method, and as we want the first one will will ask for the 0'th group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(regexResults.group(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That gives us the first number, if we wanted the whole block of numbers we can add a wildcard `'+'` which requests 1 or more instances of the proceeding character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2134567890\n"
     ]
    }
   ],
   "source": [
    "findNumbers = r'\\d+'\n",
    "regexResults = re.search(findNumbers, 'not a number, not a number, numbers 2134567890, not a number')\n",
    "print(regexResults.group(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the whole block of numbers, there are a huge number of special characters in regex, for the full description of Python's implementation look at the [re docs](https://docs.python.org/3/library/re.html) there is also a short [tutorial](https://docs.python.org/3/howto/regex.html#regex-howto).\n",
    "\n",
    "# Spidering\n",
    "\n",
    "What if we want to to get a bunch of different pages from wikipedia. We would need to get the url of each of the pages we want, usually we will want pages that are linked to by other pages, so we will need to parse pages and find the links. Right now we will be getting all the links in the body of the content analysis page.\n",
    "\n",
    "To do this we will need to find all the `<a>` (anchor) tags with `href`s (hyperlink references) inside of `<p>` tags. `href` can have many [different](http://stackoverflow.com/questions/4855168/what-is-href-and-why-is-it-used) [forms](https://en.wikipedia.org/wiki/Hyperlink#Hyperlinks_in_HTML) so dealing with them can be tricky generally though you be extracting from it absolute or relative links. An absolute link is one you can follow with out any modification, while a relative link has a base url that you are then appending. Wikipedia uses relative urls for its internal links so below is an example of dealing with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('https://en.wikipedia.org/wiki/Text_(literary_theory)', 1, 'texts'), ('https://en.wikipedia.org/wiki/Trace_evidence', 2, 'traces'), ('https://en.wikipedia.org/wiki/Abductive_reasoning', 2, 'abduction'), ('https://en.wikipedia.org/wiki/Hermeneutics', 4, 'Hermeneutics'), ('https://en.wikipedia.org/wiki/Philology', 4, 'Philology'), ('https://en.wikipedia.org/wiki/Authentication', 4, 'authenticity'), ('https://en.wikipedia.org/wiki/Mass_communication', 5, 'mass communication'), ('https://en.wikipedia.org/wiki/Harold_Lasswell', 5, 'Harold Lasswell'), ('https://en.wikipedia.org/wiki/Bernard_Berelson', 5, 'Bernard Berelson'), ('https://en.wikipedia.org/wiki/Big_data', 6, 'big data')]\n"
     ]
    }
   ],
   "source": [
    "#wikipedia_base_url = 'https://en.wikipedia.org'\n",
    "\n",
    "otherPAgeURLS = []\n",
    "#We also want to know where the links come from so we also will get:\n",
    "#the paragraph number\n",
    "#the word the link is in\n",
    "for paragraphNum, pTag in enumerate(contentPTags):\n",
    "    #we only want hrefs that link to wiki pages\n",
    "    tagLinks = pTag.findAll('a', href=re.compile('/wiki/'), class_=False)\n",
    "    for aTag in tagLinks:\n",
    "        #We need to extract the url from the <a> tag\n",
    "        relurl = aTag.get('href')\n",
    "        linkText = aTag.text\n",
    "        #wikipedia_base_url is the base we can use the urllib joining function to merge them\n",
    "        #Giving a nice structured tupe like this means we can use tuple expansion later\n",
    "        otherPAgeURLS.append((\n",
    "            urllib.parse.urljoin(wikipedia_base_url, relurl),\n",
    "            paragraphNum,\n",
    "            linkText,\n",
    "        ))\n",
    "print(otherPAgeURLS[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be adding these new texts to our DataFrame `contentParagraphsDF` so we will need to add 2 more columns to keep track of paragraph numbers and sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph-text</th>\n",
       "      <th>source</th>\n",
       "      <th>paragraph-number</th>\n",
       "      <th>source-paragraph-number</th>\n",
       "      <th>source-paragraph-text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Content analysis is \"a wide and heterogeneous ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Content analysis has come to be a sort of 'umb...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Content analysis can also be described as stud...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Despite the wide variety of options, generally...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Over the years, content analysis has been appl...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>In recent times, particularly with the advent ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Quantitative content analysis has enjoyed a re...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>6</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>7</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>8</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The method of content analysis enables the res...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>9</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Since the 1980s, content analysis has become a...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>10</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The creation of coding frames is intrinsically...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>11</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Mimetic Convergence thus aims to show the proc...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>12</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Every content analysis should depart from a hy...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>13</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>As an evaluation approach, content analysis is...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>14</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Qualitative content analysis is “a systematic,...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>15</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Holsti groups fifteen uses of content analysis...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>16</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>He also places these uses into the context of ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>17</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The following table shows fifteen uses of cont...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>18</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>According to Dr. Klaus Krippendorff, six quest...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>19</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>The assumption is that words and phrases menti...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>20</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Qualitatively, content analysis can involve an...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>21</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Normally, content analysis can only be applied...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>22</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>A further step in analysis is the distinction ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>23</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Dermot McKeone highlighted the difference betw...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>24</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>As the uncritical use of text is today widely ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>25</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Neuendorf suggests that when human coders are ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>26</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       paragraph-text  \\\n",
       "0   Content analysis is \"a wide and heterogeneous ...   \n",
       "1   Content analysis has come to be a sort of 'umb...   \n",
       "2   Content analysis can also be described as stud...   \n",
       "3   Despite the wide variety of options, generally...   \n",
       "4   Over the years, content analysis has been appl...   \n",
       "5   In recent times, particularly with the advent ...   \n",
       "6   Quantitative content analysis has enjoyed a re...   \n",
       "7                                                       \n",
       "8                                                       \n",
       "9   The method of content analysis enables the res...   \n",
       "10  Since the 1980s, content analysis has become a...   \n",
       "11  The creation of coding frames is intrinsically...   \n",
       "12  Mimetic Convergence thus aims to show the proc...   \n",
       "13  Every content analysis should depart from a hy...   \n",
       "14  As an evaluation approach, content analysis is...   \n",
       "15  Qualitative content analysis is “a systematic,...   \n",
       "16  Holsti groups fifteen uses of content analysis...   \n",
       "17  He also places these uses into the context of ...   \n",
       "18  The following table shows fifteen uses of cont...   \n",
       "19  According to Dr. Klaus Krippendorff, six quest...   \n",
       "20  The assumption is that words and phrases menti...   \n",
       "21  Qualitatively, content analysis can involve an...   \n",
       "22  Normally, content analysis can only be applied...   \n",
       "23  A further step in analysis is the distinction ...   \n",
       "24  Dermot McKeone highlighted the difference betw...   \n",
       "25  As the uncritical use of text is today widely ...   \n",
       "26  Neuendorf suggests that when human coders are ...   \n",
       "\n",
       "                                            source  paragraph-number  \\\n",
       "0   https://en.wikipedia.org/wiki/Content_analysis                 0   \n",
       "1   https://en.wikipedia.org/wiki/Content_analysis                 1   \n",
       "2   https://en.wikipedia.org/wiki/Content_analysis                 2   \n",
       "3   https://en.wikipedia.org/wiki/Content_analysis                 3   \n",
       "4   https://en.wikipedia.org/wiki/Content_analysis                 4   \n",
       "5   https://en.wikipedia.org/wiki/Content_analysis                 5   \n",
       "6   https://en.wikipedia.org/wiki/Content_analysis                 6   \n",
       "7   https://en.wikipedia.org/wiki/Content_analysis                 7   \n",
       "8   https://en.wikipedia.org/wiki/Content_analysis                 8   \n",
       "9   https://en.wikipedia.org/wiki/Content_analysis                 9   \n",
       "10  https://en.wikipedia.org/wiki/Content_analysis                10   \n",
       "11  https://en.wikipedia.org/wiki/Content_analysis                11   \n",
       "12  https://en.wikipedia.org/wiki/Content_analysis                12   \n",
       "13  https://en.wikipedia.org/wiki/Content_analysis                13   \n",
       "14  https://en.wikipedia.org/wiki/Content_analysis                14   \n",
       "15  https://en.wikipedia.org/wiki/Content_analysis                15   \n",
       "16  https://en.wikipedia.org/wiki/Content_analysis                16   \n",
       "17  https://en.wikipedia.org/wiki/Content_analysis                17   \n",
       "18  https://en.wikipedia.org/wiki/Content_analysis                18   \n",
       "19  https://en.wikipedia.org/wiki/Content_analysis                19   \n",
       "20  https://en.wikipedia.org/wiki/Content_analysis                20   \n",
       "21  https://en.wikipedia.org/wiki/Content_analysis                21   \n",
       "22  https://en.wikipedia.org/wiki/Content_analysis                22   \n",
       "23  https://en.wikipedia.org/wiki/Content_analysis                23   \n",
       "24  https://en.wikipedia.org/wiki/Content_analysis                24   \n",
       "25  https://en.wikipedia.org/wiki/Content_analysis                25   \n",
       "26  https://en.wikipedia.org/wiki/Content_analysis                26   \n",
       "\n",
       "   source-paragraph-number source-paragraph-text  \n",
       "0                     None                  None  \n",
       "1                     None                  None  \n",
       "2                     None                  None  \n",
       "3                     None                  None  \n",
       "4                     None                  None  \n",
       "5                     None                  None  \n",
       "6                     None                  None  \n",
       "7                     None                  None  \n",
       "8                     None                  None  \n",
       "9                     None                  None  \n",
       "10                    None                  None  \n",
       "11                    None                  None  \n",
       "12                    None                  None  \n",
       "13                    None                  None  \n",
       "14                    None                  None  \n",
       "15                    None                  None  \n",
       "16                    None                  None  \n",
       "17                    None                  None  \n",
       "18                    None                  None  \n",
       "19                    None                  None  \n",
       "20                    None                  None  \n",
       "21                    None                  None  \n",
       "22                    None                  None  \n",
       "23                    None                  None  \n",
       "24                    None                  None  \n",
       "25                    None                  None  \n",
       "26                    None                  None  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contentParagraphsDF['source'] = [wikipedia_content_analysis] * len(contentParagraphsDF['paragraph-text'])\n",
    "contentParagraphsDF['paragraph-number'] = range(len(contentParagraphsDF['paragraph-text']))\n",
    "contentParagraphsDF['source-paragraph-number'] = [None] * len(contentParagraphsDF['paragraph-text'])\n",
    "contentParagraphsDF['source-paragraph-text'] = [None] * len(contentParagraphsDF['paragraph-text'])\n",
    "\n",
    "contentParagraphsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can define a function to parse each linked page and add its text to our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTextFromWikiPage(targetURL, sourceParNum, sourceText):\n",
    "    #Make a dict to store data before adding it to the DataFrame\n",
    "    parsDict = {'source' : [], 'paragraph-number' : [], 'paragraph-text' : [], 'source-paragraph-number' : [],  'source-paragraph-text' : []}\n",
    "    #Now we get the page\n",
    "    r = requests.get(targetURL)\n",
    "    soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "    #enumerating gives use the paragraph number\n",
    "    for parNum, pTag in enumerate(soup.body.findAll('p')):\n",
    "        #same regex as before\n",
    "        parsDict['paragraph-text'].append(re.sub(r'\\[\\d+\\]', '', pTag.text))\n",
    "        parsDict['paragraph-number'].append(parNum)\n",
    "        parsDict['source'].append(targetURL)\n",
    "        parsDict['source-paragraph-number'].append(sourceParNum)\n",
    "        parsDict['source-paragraph-text'].append(sourceText)\n",
    "    return pandas.DataFrame(parsDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run it on our list of link tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph-number</th>\n",
       "      <th>paragraph-text</th>\n",
       "      <th>source</th>\n",
       "      <th>source-paragraph-number</th>\n",
       "      <th>source-paragraph-text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Content analysis is \"a wide and heterogeneous ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Content analysis has come to be a sort of 'umb...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Content analysis can also be described as stud...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Despite the wide variety of options, generally...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Over the years, content analysis has been appl...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>In recent times, particularly with the advent ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Quantitative content analysis has enjoyed a re...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td></td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>The method of content analysis enables the res...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Since the 1980s, content analysis has become a...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>The creation of coding frames is intrinsically...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>Mimetic Convergence thus aims to show the proc...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>Every content analysis should depart from a hy...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>As an evaluation approach, content analysis is...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>Qualitative content analysis is “a systematic,...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>Holsti groups fifteen uses of content analysis...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>He also places these uses into the context of ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>The following table shows fifteen uses of cont...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>According to Dr. Klaus Krippendorff, six quest...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>The assumption is that words and phrases menti...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>Qualitatively, content analysis can involve an...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>Normally, content analysis can only be applied...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>A further step in analysis is the distinction ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>Dermot McKeone highlighted the difference betw...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>As the uncritical use of text is today widely ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>Neuendorf suggests that when human coders are ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>In literary theory, a text is any object that ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Text_(literary_t...</td>\n",
       "      <td>1</td>\n",
       "      <td>texts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>Within the field of literary criticism, \"text\"...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Text_(literary_t...</td>\n",
       "      <td>1</td>\n",
       "      <td>texts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>Since the history of writing predates the conc...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Text_(literary_t...</td>\n",
       "      <td>1</td>\n",
       "      <td>texts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>55</td>\n",
       "      <td>The hypothesis is framed, but not asserted, in...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>56</td>\n",
       "      <td>Note that the hypothesis (\"A\") could be of a r...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>57</td>\n",
       "      <td>Peirce did not remain quite convinced about an...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>58</td>\n",
       "      <td>In 1901 Peirce wrote, \"There would be no logic...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>59</td>\n",
       "      <td>\"Consider what effects, that might conceivably...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>60</td>\n",
       "      <td>It is a method for fruitful clarification of c...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>61</td>\n",
       "      <td>Peirce came over the years to divide (philosop...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>62</td>\n",
       "      <td>Peirce had, from the start, seen the modes of ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>63</td>\n",
       "      <td>As early as 1866, Peirce held that:</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>64</td>\n",
       "      <td>1. Hypothesis (abductive inference) is inferen...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>65</td>\n",
       "      <td>In 1902, Peirce wrote that, in abduction: \"It ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>66</td>\n",
       "      <td>At the critical level Peirce examined the form...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>67</td>\n",
       "      <td>The phrase \"inference to the best explanation\"...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>68</td>\n",
       "      <td>At the methodeutical level Peirce held that a ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>69</td>\n",
       "      <td>Norwood Russell Hanson, a philosopher of scien...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>70</td>\n",
       "      <td>Further development of the concept can be foun...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>71</td>\n",
       "      <td>Applications in artificial intelligence includ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>72</td>\n",
       "      <td>In medicine, abduction can be seen as a compon...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>73</td>\n",
       "      <td>Abduction can also be used to model automated ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>74</td>\n",
       "      <td>In intelligence analysis, analysis of competin...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>75</td>\n",
       "      <td>Belief revision, the process of adapting belie...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>76</td>\n",
       "      <td>In the philosophy of science, abduction has be...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>77</td>\n",
       "      <td>In historical linguistics, abduction during la...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>78</td>\n",
       "      <td>In anthropology, Alfred Gell in his influentia...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>79</td>\n",
       "      <td>Consequently, to discover is simply to expedit...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>80</td>\n",
       "      <td>It allows any flight of imagination, provided ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>81</td>\n",
       "      <td>Methodeutic has a special interest in Abductio...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>82</td>\n",
       "      <td>.... What is good abduction? What should an ex...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>83</td>\n",
       "      <td>The mind seeks to bring the facts, as modified...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>84</td>\n",
       "      <td>Thus, twenty skillful hypotheses will ascertai...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abductive_reasoning</td>\n",
       "      <td>2</td>\n",
       "      <td>abduction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     paragraph-number                                     paragraph-text  \\\n",
       "0                   0  Content analysis is \"a wide and heterogeneous ...   \n",
       "1                   1  Content analysis has come to be a sort of 'umb...   \n",
       "2                   2  Content analysis can also be described as stud...   \n",
       "3                   3  Despite the wide variety of options, generally...   \n",
       "4                   4  Over the years, content analysis has been appl...   \n",
       "5                   5  In recent times, particularly with the advent ...   \n",
       "6                   6  Quantitative content analysis has enjoyed a re...   \n",
       "7                   7                                                      \n",
       "8                   8                                                      \n",
       "9                   9  The method of content analysis enables the res...   \n",
       "10                 10  Since the 1980s, content analysis has become a...   \n",
       "11                 11  The creation of coding frames is intrinsically...   \n",
       "12                 12  Mimetic Convergence thus aims to show the proc...   \n",
       "13                 13  Every content analysis should depart from a hy...   \n",
       "14                 14  As an evaluation approach, content analysis is...   \n",
       "15                 15  Qualitative content analysis is “a systematic,...   \n",
       "16                 16  Holsti groups fifteen uses of content analysis...   \n",
       "17                 17  He also places these uses into the context of ...   \n",
       "18                 18  The following table shows fifteen uses of cont...   \n",
       "19                 19  According to Dr. Klaus Krippendorff, six quest...   \n",
       "20                 20  The assumption is that words and phrases menti...   \n",
       "21                 21  Qualitatively, content analysis can involve an...   \n",
       "22                 22  Normally, content analysis can only be applied...   \n",
       "23                 23  A further step in analysis is the distinction ...   \n",
       "24                 24  Dermot McKeone highlighted the difference betw...   \n",
       "25                 25  As the uncritical use of text is today widely ...   \n",
       "26                 26  Neuendorf suggests that when human coders are ...   \n",
       "27                  0  In literary theory, a text is any object that ...   \n",
       "28                  1  Within the field of literary criticism, \"text\"...   \n",
       "29                  2  Since the history of writing predates the conc...   \n",
       "..                ...                                                ...   \n",
       "101                55  The hypothesis is framed, but not asserted, in...   \n",
       "102                56  Note that the hypothesis (\"A\") could be of a r...   \n",
       "103                57  Peirce did not remain quite convinced about an...   \n",
       "104                58  In 1901 Peirce wrote, \"There would be no logic...   \n",
       "105                59  \"Consider what effects, that might conceivably...   \n",
       "106                60  It is a method for fruitful clarification of c...   \n",
       "107                61  Peirce came over the years to divide (philosop...   \n",
       "108                62  Peirce had, from the start, seen the modes of ...   \n",
       "109                63                As early as 1866, Peirce held that:   \n",
       "110                64  1. Hypothesis (abductive inference) is inferen...   \n",
       "111                65  In 1902, Peirce wrote that, in abduction: \"It ...   \n",
       "112                66  At the critical level Peirce examined the form...   \n",
       "113                67  The phrase \"inference to the best explanation\"...   \n",
       "114                68  At the methodeutical level Peirce held that a ...   \n",
       "115                69  Norwood Russell Hanson, a philosopher of scien...   \n",
       "116                70  Further development of the concept can be foun...   \n",
       "117                71  Applications in artificial intelligence includ...   \n",
       "118                72  In medicine, abduction can be seen as a compon...   \n",
       "119                73  Abduction can also be used to model automated ...   \n",
       "120                74  In intelligence analysis, analysis of competin...   \n",
       "121                75  Belief revision, the process of adapting belie...   \n",
       "122                76  In the philosophy of science, abduction has be...   \n",
       "123                77  In historical linguistics, abduction during la...   \n",
       "124                78  In anthropology, Alfred Gell in his influentia...   \n",
       "125                79  Consequently, to discover is simply to expedit...   \n",
       "126                80  It allows any flight of imagination, provided ...   \n",
       "127                81  Methodeutic has a special interest in Abductio...   \n",
       "128                82  .... What is good abduction? What should an ex...   \n",
       "129                83  The mind seeks to bring the facts, as modified...   \n",
       "130                84  Thus, twenty skillful hypotheses will ascertai...   \n",
       "\n",
       "                                                source  \\\n",
       "0       https://en.wikipedia.org/wiki/Content_analysis   \n",
       "1       https://en.wikipedia.org/wiki/Content_analysis   \n",
       "2       https://en.wikipedia.org/wiki/Content_analysis   \n",
       "3       https://en.wikipedia.org/wiki/Content_analysis   \n",
       "4       https://en.wikipedia.org/wiki/Content_analysis   \n",
       "5       https://en.wikipedia.org/wiki/Content_analysis   \n",
       "6       https://en.wikipedia.org/wiki/Content_analysis   \n",
       "7       https://en.wikipedia.org/wiki/Content_analysis   \n",
       "8       https://en.wikipedia.org/wiki/Content_analysis   \n",
       "9       https://en.wikipedia.org/wiki/Content_analysis   \n",
       "10      https://en.wikipedia.org/wiki/Content_analysis   \n",
       "11      https://en.wikipedia.org/wiki/Content_analysis   \n",
       "12      https://en.wikipedia.org/wiki/Content_analysis   \n",
       "13      https://en.wikipedia.org/wiki/Content_analysis   \n",
       "14      https://en.wikipedia.org/wiki/Content_analysis   \n",
       "15      https://en.wikipedia.org/wiki/Content_analysis   \n",
       "16      https://en.wikipedia.org/wiki/Content_analysis   \n",
       "17      https://en.wikipedia.org/wiki/Content_analysis   \n",
       "18      https://en.wikipedia.org/wiki/Content_analysis   \n",
       "19      https://en.wikipedia.org/wiki/Content_analysis   \n",
       "20      https://en.wikipedia.org/wiki/Content_analysis   \n",
       "21      https://en.wikipedia.org/wiki/Content_analysis   \n",
       "22      https://en.wikipedia.org/wiki/Content_analysis   \n",
       "23      https://en.wikipedia.org/wiki/Content_analysis   \n",
       "24      https://en.wikipedia.org/wiki/Content_analysis   \n",
       "25      https://en.wikipedia.org/wiki/Content_analysis   \n",
       "26      https://en.wikipedia.org/wiki/Content_analysis   \n",
       "27   https://en.wikipedia.org/wiki/Text_(literary_t...   \n",
       "28   https://en.wikipedia.org/wiki/Text_(literary_t...   \n",
       "29   https://en.wikipedia.org/wiki/Text_(literary_t...   \n",
       "..                                                 ...   \n",
       "101  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "102  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "103  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "104  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "105  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "106  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "107  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "108  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "109  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "110  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "111  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "112  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "113  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "114  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "115  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "116  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "117  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "118  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "119  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "120  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "121  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "122  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "123  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "124  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "125  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "126  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "127  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "128  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "129  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "130  https://en.wikipedia.org/wiki/Abductive_reasoning   \n",
       "\n",
       "     source-paragraph-number source-paragraph-text  \n",
       "0                        NaN                   NaN  \n",
       "1                        NaN                   NaN  \n",
       "2                        NaN                   NaN  \n",
       "3                        NaN                   NaN  \n",
       "4                        NaN                   NaN  \n",
       "5                        NaN                   NaN  \n",
       "6                        NaN                   NaN  \n",
       "7                        NaN                   NaN  \n",
       "8                        NaN                   NaN  \n",
       "9                        NaN                   NaN  \n",
       "10                       NaN                   NaN  \n",
       "11                       NaN                   NaN  \n",
       "12                       NaN                   NaN  \n",
       "13                       NaN                   NaN  \n",
       "14                       NaN                   NaN  \n",
       "15                       NaN                   NaN  \n",
       "16                       NaN                   NaN  \n",
       "17                       NaN                   NaN  \n",
       "18                       NaN                   NaN  \n",
       "19                       NaN                   NaN  \n",
       "20                       NaN                   NaN  \n",
       "21                       NaN                   NaN  \n",
       "22                       NaN                   NaN  \n",
       "23                       NaN                   NaN  \n",
       "24                       NaN                   NaN  \n",
       "25                       NaN                   NaN  \n",
       "26                       NaN                   NaN  \n",
       "27                         1                 texts  \n",
       "28                         1                 texts  \n",
       "29                         1                 texts  \n",
       "..                       ...                   ...  \n",
       "101                        2             abduction  \n",
       "102                        2             abduction  \n",
       "103                        2             abduction  \n",
       "104                        2             abduction  \n",
       "105                        2             abduction  \n",
       "106                        2             abduction  \n",
       "107                        2             abduction  \n",
       "108                        2             abduction  \n",
       "109                        2             abduction  \n",
       "110                        2             abduction  \n",
       "111                        2             abduction  \n",
       "112                        2             abduction  \n",
       "113                        2             abduction  \n",
       "114                        2             abduction  \n",
       "115                        2             abduction  \n",
       "116                        2             abduction  \n",
       "117                        2             abduction  \n",
       "118                        2             abduction  \n",
       "119                        2             abduction  \n",
       "120                        2             abduction  \n",
       "121                        2             abduction  \n",
       "122                        2             abduction  \n",
       "123                        2             abduction  \n",
       "124                        2             abduction  \n",
       "125                        2             abduction  \n",
       "126                        2             abduction  \n",
       "127                        2             abduction  \n",
       "128                        2             abduction  \n",
       "129                        2             abduction  \n",
       "130                        2             abduction  \n",
       "\n",
       "[131 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for urlTuple in otherPAgeURLS[:3]:\n",
    "    #ignore_index means the indices will not be reset after each append\n",
    "    contentParagraphsDF = contentParagraphsDF.append(getTextFromWikiPage(*urlTuple),ignore_index=True)\n",
    "contentParagraphsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tumblr API\n",
    "\n",
    "Generally website owners do not like you scraping their sites. Scraping if done badly can act like a DOS attack so you should be careful about how often you make calls to a site. Some site though want automated tools to access their data, so they create [application programming interface (APIs)](https://en.wikipedia.org/wiki/Application_programming_interface). An API specifies a procedure for an application (or script) to access their data. Often this is though a [representational state transfer (REST)](https://en.wikipedia.org/wiki/Representational_state_transfer) web service, which just means if you make correctly formatted HTTP requests they will return nicely formatted data.\n",
    "\n",
    "A nice example for us to study is [Tumblr](https://www.tumblr.com), they have a [simple RESTful API](https://www.tumblr.com/docs/en/api/v1) that allows you to read posts without any complicated html parsing.\n",
    "\n",
    "We can get the first 20 posts from a blog by making an http GET request to `'http://{blog}.tumblr.com/api/read/json'`, were `{blog}` is the name of the target blog. Lets try and get the posts from [http://lolcats-lol-cat.tumblr.com/](http://lolcats-lol-cat.tumblr.com/) (Note the blog says at the top 'One hour one pic lolcats', but the canonical name that Tumblr uses is in the URL 'lolcats-lol-cat')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var tumblr_api_read = {\"tumblelog\":{\"title\":\"One hour one pic lolcats\",\"description\":\"\",\"name\":\"lolcats-lol-cat\",\"timezone\":\"Europe\\/Paris\",\"cname\":false,\"feeds\":[]},\"posts-start\":0,\"posts-total\":2684,\"posts-type\":false,\"posts\":[{\"id\":\"155320542732\",\"url\":\"http:\\/\\/lolcats-lol-cat.tumblr.com\\/post\\/155320542732\",\"url-with-slug\":\"http:\\/\\/lolcats-lol-cat.tumblr.com\\/post\\/155320542732\\/typical-cat\",\"type\":\"photo\",\"date-gmt\":\"2017-01-03 01:00:35 GMT\",\"date\":\"Tue, 03 Jan 2017 02:00:35\",\"bookmarklet\":0,\"mobile\":0,\"feed-item\":\"\",\"from-feed-id\":0,\"unix-timestamp\":1483405235,\"format\":\"html\",\"reblog-key\":\"JsYuDxeO\",\"slug\":\"typical-cat\",\"is-submission\":false,\"like-button\":\"<div class=\\\"like_button\\\" data-post-id=\\\"155320542732\\\" data-blog-name=\\\"lolcats-lol-cat\\\" id=\\\"like_button_155320542732\\\"><iframe id=\\\"like_iframe_155320542732\\\" src=\\\"http:\\/\\/assets.tumblr.com\\/assets\\/html\\/like_iframe.html?_v=399aee1ea58b6007c2190a8138f100e5#name=lolcats-lol-cat&amp;post_id=155320542732&amp;color=black&\n"
     ]
    }
   ],
   "source": [
    "tumblrAPItarget = 'http://{}.tumblr.com/api/read/json'\n",
    "\n",
    "r = requests.get(tumblrAPItarget.format('lolcats-lol-cat'))\n",
    "\n",
    "print(r.text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might not look very good, but it has a lot fewer angle braces than html, which is nice. What we have is [JSON](https://en.wikipedia.org/wiki/JSON) a 'human readable' text based data transmission format based on javascript. Luckily, we can convert it to a python `dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['posts-start', 'posts-type', 'posts', 'posts-total', 'tumblelog'])\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#We need to load only the stuff between the curly braces\n",
    "d = json.loads(r.text[len('var tumblr_api_read = '):-2])\n",
    "print(d.keys())\n",
    "print(len(d['posts']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we read the [API specification](https://www.tumblr.com/docs/en/api/v1), we will see there are a lot of things we can get if, we add things to our GET request. First we can get posts by their id number, lets get post `146020177084`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(tumblrAPItarget.format('lolcats-lol-cat'), params = {'id' : 146020177084})\n",
    "d = json.loads(r.text[len('var tumblr_api_read = '):-2])\n",
    "d['posts'][0].keys()\n",
    "d['posts'][0]['photo-url-1280']\n",
    "\n",
    "with open('lolcat.gif', 'wb') as f:\n",
    "    gifRequest = requests.get(d['posts'][0]['photo-url-1280'], stream = True)\n",
    "    f.write(gifRequest.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='lolcat.gif'>\n",
    "\n",
    "Such beauty, now we could get the text from all the posts as well as some metadata, like the post date, caption or the tags. But, we could instead get the links to all the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>photo-type</th>\n",
       "      <th>photo-url</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tue, 03 Jan 2017 02:00:35</td>\n",
       "      <td>155320542732</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/7567ddcbee4af447779...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mon, 02 Jan 2017 22:00:53</td>\n",
       "      <td>155310651518</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/a2a9e83e75b3d8915ac...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sat, 31 Dec 2016 08:00:34</td>\n",
       "      <td>155198080351</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/0c59f1654b492097c58...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sat, 31 Dec 2016 06:00:33</td>\n",
       "      <td>155194054871</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/e1c705ca985b11c1e1a...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tue, 20 Dec 2016 22:00:44</td>\n",
       "      <td>154733970182</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/bfc00e121ede86ec9d4...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tue, 20 Dec 2016 20:00:31</td>\n",
       "      <td>154729675124</td>\n",
       "      <td>png</td>\n",
       "      <td>http://68.media.tumblr.com/5f7e855d512ab2b2dc4...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mon, 12 Dec 2016 22:00:33</td>\n",
       "      <td>154390040542</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/6fb9ab5f5b041be83d7...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mon, 12 Dec 2016 20:00:41</td>\n",
       "      <td>154385661374</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/ba07f00e8ce8e4da765...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Fri, 09 Dec 2016 04:00:15</td>\n",
       "      <td>154230465752</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/7530c2210673a58e392...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Fri, 09 Dec 2016 02:00:30</td>\n",
       "      <td>154226290124</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/b2a5e62ef55a6d176c3...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Tue, 06 Dec 2016 14:00:22</td>\n",
       "      <td>154117927413</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/cc44f2ff65366ec6b81...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Tue, 06 Dec 2016 12:00:43</td>\n",
       "      <td>154115515601</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/fef80b257374162c757...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Tue, 06 Dec 2016 10:00:40</td>\n",
       "      <td>154113363187</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/e2fa3b7422a9873dff3...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Tue, 06 Dec 2016 08:00:15</td>\n",
       "      <td>154110609304</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/54068be23b407110229...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Tue, 06 Dec 2016 06:00:14</td>\n",
       "      <td>154106783936</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/3bb935758ee6eb31cdf...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Tue, 06 Dec 2016 04:00:40</td>\n",
       "      <td>154102394657</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/4592f23c575e7f35928...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Tue, 06 Dec 2016 02:00:44</td>\n",
       "      <td>154098070460</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/ea71fe64c5b5dc26c28...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Mon, 05 Dec 2016 22:00:45</td>\n",
       "      <td>154089157891</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://68.media.tumblr.com/25be4d56ba617656ce4...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Mon, 21 Nov 2016 18:00:28</td>\n",
       "      <td>153478120128</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/fc27d6b324431b73e38...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny, cats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Mon, 14 Nov 2016 02:00:19</td>\n",
       "      <td>153149865341</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/b9cc0ab4e1a591eb63f...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny, cats, momen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Sun, 13 Nov 2016 22:00:55</td>\n",
       "      <td>153140437184</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/72b1a939e7ffed2f508...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny, follow, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Sun, 13 Nov 2016 20:00:28</td>\n",
       "      <td>153135623847</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/7fd52a08336a603e5d7...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny, thevideoboo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Sun, 13 Nov 2016 18:00:40</td>\n",
       "      <td>153131026174</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/15a1872f6b0001023c8...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny, what, kitty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Sun, 13 Nov 2016 16:00:28</td>\n",
       "      <td>153126814277</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/3d5803596d2cbc34cbd...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Sun, 13 Nov 2016 14:00:34</td>\n",
       "      <td>153123476159</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/2818710c2b368495adb...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Sun, 13 Nov 2016 12:00:37</td>\n",
       "      <td>153120951284</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/932da12fc804c2b79a2...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Sun, 13 Nov 2016 10:00:29</td>\n",
       "      <td>153118440973</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/1d1f6103c8bb6a57acd...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Sun, 13 Nov 2016 08:00:29</td>\n",
       "      <td>153115497731</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/044f8541d704e27acd9...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Sun, 13 Nov 2016 06:00:14</td>\n",
       "      <td>153111780742</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/8567878ac6dd3e36ba3...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Sun, 13 Nov 2016 04:00:17</td>\n",
       "      <td>153107791793</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/fd4a2ac424cbbc50fd6...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Sun, 13 Nov 2016 02:00:40</td>\n",
       "      <td>153103740161</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/c4ecd71def2516a11f8...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Sat, 12 Nov 2016 22:01:00</td>\n",
       "      <td>153094930509</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/df53d0485f126aa0cbe...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Sat, 12 Nov 2016 20:00:46</td>\n",
       "      <td>153090619967</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/4c591d205c9f5580e86...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Sat, 12 Nov 2016 18:00:54</td>\n",
       "      <td>153086322161</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/035ce8c0e0316cb8ffe...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Sat, 12 Nov 2016 16:00:39</td>\n",
       "      <td>153082328288</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/b11a5db582408e95f84...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Sat, 12 Nov 2016 14:00:36</td>\n",
       "      <td>153079060185</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/f38416a11969cef8367...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Sat, 12 Nov 2016 12:00:23</td>\n",
       "      <td>153076629202</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/2031ee8b9f33a1f6aa9...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Sat, 12 Nov 2016 10:00:33</td>\n",
       "      <td>153074321121</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/ed0c4a837a42b81e8f1...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Sat, 12 Nov 2016 08:00:30</td>\n",
       "      <td>153071640227</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/973ba620c9d15e67637...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Sat, 12 Nov 2016 06:00:30</td>\n",
       "      <td>153068105943</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/7ac28554d3daba3caef...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Sat, 12 Nov 2016 04:00:23</td>\n",
       "      <td>153064144065</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/1c9054346b7f3c50543...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Sat, 12 Nov 2016 02:00:28</td>\n",
       "      <td>153060133643</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/e37f99e5e1fc6e853d2...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Fri, 11 Nov 2016 22:00:57</td>\n",
       "      <td>153051556514</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/b1c67cf10c177f75b35...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Fri, 11 Nov 2016 20:00:32</td>\n",
       "      <td>153047244506</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/fc85e02bc4da0d3ec8d...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Fri, 11 Nov 2016 18:00:41</td>\n",
       "      <td>153043113080</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/44ba278783332ebd55a...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Fri, 11 Nov 2016 16:00:14</td>\n",
       "      <td>153039315888</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/48e43d0fe83a1228a81...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Fri, 11 Nov 2016 14:00:35</td>\n",
       "      <td>153036162147</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/9761ee0e15548c0985b...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Fri, 11 Nov 2016 12:00:18</td>\n",
       "      <td>153033848511</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/daef780e9dd29afc0d3...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Fri, 11 Nov 2016 10:00:14</td>\n",
       "      <td>153031789978</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/6c41dc356470e641716...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Fri, 11 Nov 2016 08:00:12</td>\n",
       "      <td>153029173175</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://68.media.tumblr.com/75437ac22da8bbf6af1...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         date            id photo-type  \\\n",
       "0   Tue, 03 Jan 2017 02:00:35  155320542732        jpg   \n",
       "1   Mon, 02 Jan 2017 22:00:53  155310651518        jpg   \n",
       "2   Sat, 31 Dec 2016 08:00:34  155198080351        jpg   \n",
       "3   Sat, 31 Dec 2016 06:00:33  155194054871        jpg   \n",
       "4   Tue, 20 Dec 2016 22:00:44  154733970182        jpg   \n",
       "5   Tue, 20 Dec 2016 20:00:31  154729675124        png   \n",
       "6   Mon, 12 Dec 2016 22:00:33  154390040542        jpg   \n",
       "7   Mon, 12 Dec 2016 20:00:41  154385661374        jpg   \n",
       "8   Fri, 09 Dec 2016 04:00:15  154230465752        gif   \n",
       "9   Fri, 09 Dec 2016 02:00:30  154226290124        gif   \n",
       "10  Tue, 06 Dec 2016 14:00:22  154117927413        jpg   \n",
       "11  Tue, 06 Dec 2016 12:00:43  154115515601        jpg   \n",
       "12  Tue, 06 Dec 2016 10:00:40  154113363187        jpg   \n",
       "13  Tue, 06 Dec 2016 08:00:15  154110609304        jpg   \n",
       "14  Tue, 06 Dec 2016 06:00:14  154106783936        jpg   \n",
       "15  Tue, 06 Dec 2016 04:00:40  154102394657        gif   \n",
       "16  Tue, 06 Dec 2016 02:00:44  154098070460        jpg   \n",
       "17  Mon, 05 Dec 2016 22:00:45  154089157891        jpg   \n",
       "18  Mon, 21 Nov 2016 18:00:28  153478120128        gif   \n",
       "19  Mon, 14 Nov 2016 02:00:19  153149865341        gif   \n",
       "20  Sun, 13 Nov 2016 22:00:55  153140437184        gif   \n",
       "21  Sun, 13 Nov 2016 20:00:28  153135623847        gif   \n",
       "22  Sun, 13 Nov 2016 18:00:40  153131026174        gif   \n",
       "23  Sun, 13 Nov 2016 16:00:28  153126814277        gif   \n",
       "24  Sun, 13 Nov 2016 14:00:34  153123476159        gif   \n",
       "25  Sun, 13 Nov 2016 12:00:37  153120951284        gif   \n",
       "26  Sun, 13 Nov 2016 10:00:29  153118440973        gif   \n",
       "27  Sun, 13 Nov 2016 08:00:29  153115497731        gif   \n",
       "28  Sun, 13 Nov 2016 06:00:14  153111780742        gif   \n",
       "29  Sun, 13 Nov 2016 04:00:17  153107791793        gif   \n",
       "30  Sun, 13 Nov 2016 02:00:40  153103740161        gif   \n",
       "31  Sat, 12 Nov 2016 22:01:00  153094930509        gif   \n",
       "32  Sat, 12 Nov 2016 20:00:46  153090619967        gif   \n",
       "33  Sat, 12 Nov 2016 18:00:54  153086322161        gif   \n",
       "34  Sat, 12 Nov 2016 16:00:39  153082328288        gif   \n",
       "35  Sat, 12 Nov 2016 14:00:36  153079060185        gif   \n",
       "36  Sat, 12 Nov 2016 12:00:23  153076629202        gif   \n",
       "37  Sat, 12 Nov 2016 10:00:33  153074321121        gif   \n",
       "38  Sat, 12 Nov 2016 08:00:30  153071640227        gif   \n",
       "39  Sat, 12 Nov 2016 06:00:30  153068105943        gif   \n",
       "40  Sat, 12 Nov 2016 04:00:23  153064144065        gif   \n",
       "41  Sat, 12 Nov 2016 02:00:28  153060133643        gif   \n",
       "42  Fri, 11 Nov 2016 22:00:57  153051556514        gif   \n",
       "43  Fri, 11 Nov 2016 20:00:32  153047244506        gif   \n",
       "44  Fri, 11 Nov 2016 18:00:41  153043113080        gif   \n",
       "45  Fri, 11 Nov 2016 16:00:14  153039315888        gif   \n",
       "46  Fri, 11 Nov 2016 14:00:35  153036162147        gif   \n",
       "47  Fri, 11 Nov 2016 12:00:18  153033848511        gif   \n",
       "48  Fri, 11 Nov 2016 10:00:14  153031789978        gif   \n",
       "49  Fri, 11 Nov 2016 08:00:12  153029173175        gif   \n",
       "\n",
       "                                            photo-url  \\\n",
       "0   http://68.media.tumblr.com/7567ddcbee4af447779...   \n",
       "1   http://68.media.tumblr.com/a2a9e83e75b3d8915ac...   \n",
       "2   http://68.media.tumblr.com/0c59f1654b492097c58...   \n",
       "3   http://68.media.tumblr.com/e1c705ca985b11c1e1a...   \n",
       "4   http://68.media.tumblr.com/bfc00e121ede86ec9d4...   \n",
       "5   http://68.media.tumblr.com/5f7e855d512ab2b2dc4...   \n",
       "6   http://68.media.tumblr.com/6fb9ab5f5b041be83d7...   \n",
       "7   http://68.media.tumblr.com/ba07f00e8ce8e4da765...   \n",
       "8   http://68.media.tumblr.com/7530c2210673a58e392...   \n",
       "9   http://68.media.tumblr.com/b2a5e62ef55a6d176c3...   \n",
       "10  http://68.media.tumblr.com/cc44f2ff65366ec6b81...   \n",
       "11  http://68.media.tumblr.com/fef80b257374162c757...   \n",
       "12  http://68.media.tumblr.com/e2fa3b7422a9873dff3...   \n",
       "13  http://68.media.tumblr.com/54068be23b407110229...   \n",
       "14  http://68.media.tumblr.com/3bb935758ee6eb31cdf...   \n",
       "15  http://68.media.tumblr.com/4592f23c575e7f35928...   \n",
       "16  http://68.media.tumblr.com/ea71fe64c5b5dc26c28...   \n",
       "17  http://68.media.tumblr.com/25be4d56ba617656ce4...   \n",
       "18  http://68.media.tumblr.com/fc27d6b324431b73e38...   \n",
       "19  http://68.media.tumblr.com/b9cc0ab4e1a591eb63f...   \n",
       "20  http://68.media.tumblr.com/72b1a939e7ffed2f508...   \n",
       "21  http://68.media.tumblr.com/7fd52a08336a603e5d7...   \n",
       "22  http://68.media.tumblr.com/15a1872f6b0001023c8...   \n",
       "23  http://68.media.tumblr.com/3d5803596d2cbc34cbd...   \n",
       "24  http://68.media.tumblr.com/2818710c2b368495adb...   \n",
       "25  http://68.media.tumblr.com/932da12fc804c2b79a2...   \n",
       "26  http://68.media.tumblr.com/1d1f6103c8bb6a57acd...   \n",
       "27  http://68.media.tumblr.com/044f8541d704e27acd9...   \n",
       "28  http://68.media.tumblr.com/8567878ac6dd3e36ba3...   \n",
       "29  http://68.media.tumblr.com/fd4a2ac424cbbc50fd6...   \n",
       "30  http://68.media.tumblr.com/c4ecd71def2516a11f8...   \n",
       "31  http://68.media.tumblr.com/df53d0485f126aa0cbe...   \n",
       "32  http://68.media.tumblr.com/4c591d205c9f5580e86...   \n",
       "33  http://68.media.tumblr.com/035ce8c0e0316cb8ffe...   \n",
       "34  http://68.media.tumblr.com/b11a5db582408e95f84...   \n",
       "35  http://68.media.tumblr.com/f38416a11969cef8367...   \n",
       "36  http://68.media.tumblr.com/2031ee8b9f33a1f6aa9...   \n",
       "37  http://68.media.tumblr.com/ed0c4a837a42b81e8f1...   \n",
       "38  http://68.media.tumblr.com/973ba620c9d15e67637...   \n",
       "39  http://68.media.tumblr.com/7ac28554d3daba3caef...   \n",
       "40  http://68.media.tumblr.com/1c9054346b7f3c50543...   \n",
       "41  http://68.media.tumblr.com/e37f99e5e1fc6e853d2...   \n",
       "42  http://68.media.tumblr.com/b1c67cf10c177f75b35...   \n",
       "43  http://68.media.tumblr.com/fc85e02bc4da0d3ec8d...   \n",
       "44  http://68.media.tumblr.com/44ba278783332ebd55a...   \n",
       "45  http://68.media.tumblr.com/48e43d0fe83a1228a81...   \n",
       "46  http://68.media.tumblr.com/9761ee0e15548c0985b...   \n",
       "47  http://68.media.tumblr.com/daef780e9dd29afc0d3...   \n",
       "48  http://68.media.tumblr.com/6c41dc356470e641716...   \n",
       "49  http://68.media.tumblr.com/75437ac22da8bbf6af1...   \n",
       "\n",
       "                                                 tags  \n",
       "0                   [cat, cats, lol, lolcat, lolcats]  \n",
       "1                   [cat, cats, lol, lolcat, lolcats]  \n",
       "2                   [cat, cats, lol, lolcat, lolcats]  \n",
       "3                   [cat, cats, lol, lolcat, lolcats]  \n",
       "4                   [cat, cats, lol, lolcat, lolcats]  \n",
       "5                   [cat, cats, lol, lolcat, lolcats]  \n",
       "6                   [cat, cats, lol, lolcat, lolcats]  \n",
       "7                   [cat, cats, lol, lolcat, lolcats]  \n",
       "8                  [gif, lolcat, lolcats, cat, funny]  \n",
       "9                  [gif, lolcat, lolcats, cat, funny]  \n",
       "10                  [cat, cats, lol, lolcat, lolcats]  \n",
       "11                  [cat, cats, lol, lolcat, lolcats]  \n",
       "12                  [cat, cats, lol, lolcat, lolcats]  \n",
       "13                  [cat, cats, lol, lolcat, lolcats]  \n",
       "14                  [cat, cats, lol, lolcat, lolcats]  \n",
       "15                  [cat, cats, lol, lolcat, lolcats]  \n",
       "16                  [cat, cats, lol, lolcat, lolcats]  \n",
       "17                  [cat, cats, lol, lolcat, lolcats]  \n",
       "18           [gif, lolcat, lolcats, cat, funny, cats]  \n",
       "19  [gif, lolcat, lolcats, cat, funny, cats, momen...  \n",
       "20  [gif, lolcat, lolcats, cat, funny, follow, the...  \n",
       "21  [gif, lolcat, lolcats, cat, funny, thevideoboo...  \n",
       "22  [gif, lolcat, lolcats, cat, funny, what, kitty...  \n",
       "23                 [gif, lolcat, lolcats, cat, funny]  \n",
       "24                 [gif, lolcat, lolcats, cat, funny]  \n",
       "25                 [gif, lolcat, lolcats, cat, funny]  \n",
       "26                 [gif, lolcat, lolcats, cat, funny]  \n",
       "27                 [gif, lolcat, lolcats, cat, funny]  \n",
       "28                 [gif, lolcat, lolcats, cat, funny]  \n",
       "29                 [gif, lolcat, lolcats, cat, funny]  \n",
       "30                 [gif, lolcat, lolcats, cat, funny]  \n",
       "31                 [gif, lolcat, lolcats, cat, funny]  \n",
       "32                 [gif, lolcat, lolcats, cat, funny]  \n",
       "33                 [gif, lolcat, lolcats, cat, funny]  \n",
       "34                 [gif, lolcat, lolcats, cat, funny]  \n",
       "35                 [gif, lolcat, lolcats, cat, funny]  \n",
       "36                 [gif, lolcat, lolcats, cat, funny]  \n",
       "37                 [gif, lolcat, lolcats, cat, funny]  \n",
       "38                 [gif, lolcat, lolcats, cat, funny]  \n",
       "39                 [gif, lolcat, lolcats, cat, funny]  \n",
       "40                 [gif, lolcat, lolcats, cat, funny]  \n",
       "41                 [gif, lolcat, lolcats, cat, funny]  \n",
       "42                 [gif, lolcat, lolcats, cat, funny]  \n",
       "43                 [gif, lolcat, lolcats, cat, funny]  \n",
       "44                 [gif, lolcat, lolcats, cat, funny]  \n",
       "45                 [gif, lolcat, lolcats, cat, funny]  \n",
       "46                 [gif, lolcat, lolcats, cat, funny]  \n",
       "47                 [gif, lolcat, lolcats, cat, funny]  \n",
       "48                 [gif, lolcat, lolcats, cat, funny]  \n",
       "49                 [gif, lolcat, lolcats, cat, funny]  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Putting a max incase the blog has millions of images\n",
    "#The given max will be rounded up to the nearest multiple of 50\n",
    "def tumblrImageScrape(blogName, maxImages = 200):\n",
    "    #Restating this here so the function isn't dependent on any external variables\n",
    "    tumblrAPItarget = 'http://{}.tumblr.com/api/read/json'\n",
    "\n",
    "    #There are a bunch of possible locations for the photo url\n",
    "    possiblePhotoSuffixes = [1280, 500, 400, 250, 100]\n",
    "\n",
    "    #These are the pieces of information we will be gathering,\n",
    "    #at the end we will convert this to a DataFrame.\n",
    "    #There are a few other ones we could get like the captions\n",
    "    #you can read the Tumblr documentation to learn how to get them\n",
    "    #https://www.tumblr.com/docs/en/api/v1\n",
    "    postsData = {\n",
    "        'id' : [],\n",
    "        'photo-url' : [],\n",
    "        'date' : [],\n",
    "        'tags' : [],\n",
    "        'photo-type' : []\n",
    "    }\n",
    "\n",
    "    #Tumblr limits us to a max of 50 posts per request\n",
    "    for requestNum in range(maxImages // 50):\n",
    "        requestParams = {\n",
    "            'start' : requestNum * 50,\n",
    "            'num' : 50,\n",
    "            'type' : 'photo'\n",
    "        }\n",
    "        r = requests.get(tumblrAPItarget.format(blogName), params = requestParams)\n",
    "        requestDict = json.loads(r.text[len('var tumblr_api_read = '):-2])\n",
    "        for postDict in requestDict['posts']:\n",
    "            #We are dealing with uncleaned data, we can't trust it.\n",
    "            #Specifically, not all posts are guaranteed to have the fields we want\n",
    "            try:\n",
    "                postsData['id'].append(postDict['id'])\n",
    "                postsData['date'].append(postDict['date'])\n",
    "                postsData['tags'].append(postDict['tags'])\n",
    "            except KeyError as e:\n",
    "                raise KeyError(\"Post {} from {} is missing: {}\".format(postDict['id'], blogName, e))\n",
    "\n",
    "            foundSuffix = False\n",
    "            for suffix in possiblePhotoSuffixes:\n",
    "                try:\n",
    "                    photoURL = postDict['photo-url-{}'.format(suffix)]\n",
    "                    postsData['photo-url'].append(photoURL)\n",
    "                    postsData['photo-type'].append(photoURL.split('.')[-1])\n",
    "                    foundSuffix = True\n",
    "                    break\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            if not foundSuffix:\n",
    "                #Make sure your error messages are useful\n",
    "                #You will be one of the users\n",
    "                raise KeyError(\"Post {} from {} is missing a photo url\".format(postDict['id'], blogName))\n",
    "\n",
    "    return pandas.DataFrame(postsData)\n",
    "tumblrImageScrape('lolcats-lol-cat', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the urls of a bunch of images and can run OCR on them.\n",
    "\n",
    "## Stuff for the v2 Tumblr API\n",
    "\n",
    "probably unnecessary\n",
    "\n",
    "+ Consumer Key: TgqpubaBeckUPRHWUCTHIe2DzGYyZ0hXYFenh2tiyZMGv874h8\n",
    "+ Secret Key:  GTXHKip2c8TJyMz9A2iRhrV1cx03FSaSaznXGoVvCW2Fx5lyCv\n",
    "\n",
    "[https://www.tumblr.com/docs/en/api/v2](https://www.tumblr.com/docs/en/api/v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the most recent 20 posts\n",
    "target = 'http://procedural-generation.tumblr.com/api/read/json'\n",
    "\n",
    "r = requests.get(target)\n",
    "\n",
    "d = json.loads(r.text[len('var tumblr_api_read = '):-2])\n",
    "\n",
    "#get a specific post\n",
    "\n",
    "r = requests.get(target, params = {'id' : '152256405098'})\n",
    "\n",
    "d = json.loads(r.text[len('var tumblr_api_read = '):-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nREQUEST_TOKEN_URL = \\'http://www.tumblr.com/oauth/request_token\\'\\nAUTHORIZATION_URL = \\'http://www.tumblr.com/oauth/authorize\\'\\nACCESS_TOKEN_URL = \\'http://www.tumblr.com/oauth/access_token\\'\\n\\ntumblrKey = \\'TgqpubaBeckUPRHWUCTHIe2DzGYyZ0hXYFenh2tiyZMGv874h8\\'\\ntumblrSecret = \\'GTXHKip2c8TJyMz9A2iRhrV1cx03FSaSaznXGoVvCW2Fx5lyCv\\'\\nconsumer = oauth2.Consumer(tumblrKey, tumblrSecret)\\nclient = oauth2.Client(consumer)\\n\\n#The token is part of content, which is provided as a binary query string\\nresponseDict, content = client.request(REQUEST_TOKEN_URL, \"GET\")\\ncontent = urllib.parse.parse_qs(content.decode(\\'utf-8\\'))\\noathToken = content[\\'oauth_token\\'][0]\\noathTokenSecret = content[\\'oauth_token_secret\\'][0]\\n\\nprint(\"The OATH token is: {}\\nThe OATH token secret is: {}\".format(oathToken, oathTokenSecret))\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "REQUEST_TOKEN_URL = 'http://www.tumblr.com/oauth/request_token'\n",
    "AUTHORIZATION_URL = 'http://www.tumblr.com/oauth/authorize'\n",
    "ACCESS_TOKEN_URL = 'http://www.tumblr.com/oauth/access_token'\n",
    "\n",
    "tumblrKey = 'TgqpubaBeckUPRHWUCTHIe2DzGYyZ0hXYFenh2tiyZMGv874h8'\n",
    "tumblrSecret = 'GTXHKip2c8TJyMz9A2iRhrV1cx03FSaSaznXGoVvCW2Fx5lyCv'\n",
    "consumer = oauth2.Consumer(tumblrKey, tumblrSecret)\n",
    "client = oauth2.Client(consumer)\n",
    "\n",
    "#The token is part of content, which is provided as a binary query string\n",
    "responseDict, content = client.request(REQUEST_TOKEN_URL, \"GET\")\n",
    "content = urllib.parse.parse_qs(content.decode('utf-8'))\n",
    "oathToken = content['oauth_token'][0]\n",
    "oathTokenSecret = content['oauth_token_secret'][0]\n",
    "\n",
    "print(\"The OATH token is: {}\\nThe OATH token secret is: {}\".format(oathToken, oathTokenSecret))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR\n",
    "\n",
    "Something about subprocess\n",
    "\n",
    "`pytesseract` works but requires tesseract binary\n",
    "\n",
    "# Files\n",
    "\n",
    "What if the text we want isn't on a webpage? There are a many other sources of text available.\n",
    "\n",
    "## Raw text\n",
    "\n",
    "The most basic form of storing text is as a _raw text_ document. Source code (`.py`, `.r`, etc) is usually raw text as are text files (`.txt`) and many other things. Opening an unknown file with a text editor is often a great way of learning what the file is.\n",
    "\n",
    "We can create a text file with the `open()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example_text_file = 'sometextfile.txt'\n",
    "#stringToWrite = 'A line\\nAnother line\\nA line with a few unusual symbols \\u2421 \\u241B \\u20A0 \\u20A1 \\u20A2 \\u20A3 \\u0D60\\n'\n",
    "stringToWrite = 'A line\\nAnother line\\nA line with a few unusual symbols ␡ ␛ ₠ ₡ ₢ ₣ ൠ\\n'\n",
    "\n",
    "with open(example_text_file, mode = 'w', encoding='utf-8') as f:\n",
    "    f.write(stringToWrite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice though the `encoding='utf-8'` argument, the encoding specifies how we map the bits from the file to the glyphs (and whitespace characters like tab (`'\\t'`) or newline (`'\\n'`)) on the screen. When dealing only with latin letters, arabic numerals and the other symbols on America keyboards you usually do not have to worry about encodings as the ones used today are backwards compatible with [ASCII](https://en.wikipedia.org/wiki/ASCII) which gives the binary representation of 128 characters.\n",
    "\n",
    "Some people though use other characters. To solve this there is [Unicode](https://en.wikipedia.org/wiki/Unicode) which gives numbers to symbols, e.g. 041 is `'A'` and 03A3 is `'Σ'` (number starting with 0 indicates they are hexadecimal), often non-ASCII characters are called Unicode characters. Unfortunately there are many ways used to map combinations of bits to Unicode symbols. The ones you are likely to encounter are called by Python _utf-8_, _utf-16_ and _latin-1_. _utf-8_ is the standard for Linux and Mac OS while both _utf-16_ and _latin-1_ are used by windows. If you use the wrong encoding characters can appear wrong, sometimes change in number or Python could raise an exception. Lets see what happens when we open the file we just created with different encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is with the correct encoding:\n",
      "A line\n",
      "Another line\n",
      "A line with a few unusual symbols ␡ ␛ ₠ ₡ ₢ ₣ ൠ\n",
      "\n",
      "This is with the wrong encoding:\n",
      "A line\n",
      "Another line\n",
      "A line with a few unusual symbols â¡ â â  â¡ â¢ â£ àµ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(example_text_file, encoding='utf-8') as f:\n",
    "    print(\"This is with the correct encoding:\")\n",
    "    print(f.read())\n",
    "\n",
    "with open(example_text_file, encoding='latin-1') as f:\n",
    "    print(\"This is with the wrong encoding:\")\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that with _latin-1_ the unicode characters are mixed up and there are too many of them. You need to keep in mind encoding when obtaining text files, as determining the encoding can sometime be a lot of work.\n",
    "\n",
    "## PDF\n",
    "\n",
    "Another common way text will be stored is in a PDF file. First we will download a pdf in Python. To do that lets grab a chapter from\n",
    "_Speech and Language Processing_, chapter 21 is on Information Extraction which seems apt. It is stored as a pdf at [https://web.stanford.edu/~jurafsky/slp3/21.pdf](https://web.stanford.edu/~jurafsky/slp3/21.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%PDF-1.5\n",
      "%����\n",
      "97 0 obj\n",
      "<<\n",
      "/Length 2988      \n",
      "/Filter /FlateDecode\n",
      ">>\n",
      "stream\n",
      "xڝYY��\u0011~�_��T,U�\u0018\u0012����f<{����z\\[)o\u001e",
      "(\n",
      "���!�������n��4�J�/b��\u0000\u001a}7�9\u0007�s������o���I�,�|�a�d�\u001b\u0002J��M��y�9?�~<i]\u001c",
      "כHE���\t�}�\u001c",
      "���e��k\u000b",
      "��esp�\u001bߋ�շyS�Jf�\u001b�|�?>��\u0006�׺\u0017�+��y7����==w��8����\u0013\u0004�M<�x�\n",
      "\u0002߉\u0003�M�Q�k�k�tW�C�6덂\n",
      "�\u001d",
      "����\u0005�拍���Y\u001c",
      "��o乤�8s\u0019���Fy-��\u0005���g��vGvM ����3���}���[+o�y�\u001b\u0018�\u000fשnٰ|7N\u0012\u00076\u0003U\u001aŽ�f��\u0019��l����D�\u000eD��|[i�n��MY��f�3��W�n`�>���X!~lڧ��>��K�7�\n",
      "��\u000fP�z\u0013�\u0018��\u0007���s;\n",
      "�zx����\u0005�t��|ӎ�\u000e��n\u001d",
      "��\u0010�V{�A\u0011v@�Wt:�!bc!*�\u0013��>�\u0014\u000e���΋�@R\u001a�\bQ`��_�A���Aл�y\u001d",
      "G+J���z[��\\[G�\u00041-\u0019r��U�m�A�|��'�2\n"
     ]
    }
   ],
   "source": [
    "#information_extraction_pdf = 'https://web.stanford.edu/~jurafsky/slp3/21.pdf'\n",
    "\n",
    "infoExtractionRequest = requests.get(information_extraction_pdf, stream=True)\n",
    "print(infoExtractionRequest.text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It says `'pdf'`, so thats a good sign, the rest though looks like we are having issues with an encoding. The random characters are not though caused by our encoding being wrong they are cause by there not being an encoding for those parts at all. PDFs are nominally binary files, meaning there are sections of binary that are specific to pdf and nothing else so you need something that knows about pdf to read them. To do that we will be using [`PyPDF2`](https://github.com/mstamy2/PyPDF2) which is a PDF processing library for Python 3.\n",
    "\n",
    "**NOTE** maybe use `PyPDF2` or `slate`\n",
    "\n",
    "Because PDFs are a very complicated file format pdfminer requires a large amount of boilerplate code to extract text, we have written a function that takes in an open PDF file and returns the text so you don't have to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readPDF(pdfFile):\n",
    "    #Based on code from http://stackoverflow.com/a/20905381/4955164\n",
    "    #Using utf-8, if there are a bunch of random symbols try changing this\n",
    "    codec = 'utf-8'\n",
    "    rsrcmgr = pdfminer.pdfinterp.PDFResourceManager()\n",
    "    retstr = io.StringIO()\n",
    "    layoutParams = pdfminer.layout.LAParams()\n",
    "    device = pdfminer.converter.TextConverter(rsrcmgr, retstr, laparams = layoutParams, codec = codec)\n",
    "    #We need a device and an interpreter\n",
    "    interpreter = pdfminer.pdfinterp.PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = ''\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "    for page in pdfminer.pdfpage.PDFPage.get_pages(pdfFile, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "    device.close()\n",
    "    returnedString = retstr.getvalue()\n",
    "    retstr.close()\n",
    "    return returnedString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first we need to take the response object and convert it into a 'file like' object so that pdfminer can read it. To do this we will use `io`'s `BytesIO`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "infoExtractionBytes = io.BytesIO(infoExtractionRequest.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can give it to pdfminer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech and Language Processing. Daniel Jurafsky & James H. Martin.\n",
      "rights reserved.\n",
      "\n",
      "Draft of November 7, 2016.\n",
      "\n",
      "Copyright c(cid:13) 2016.\n",
      "\n",
      "All\n",
      "\n",
      "CHAPTER\n",
      "\n",
      "21 Information Extraction\n",
      "\n",
      "I am the very model of a modern Major-General,\n",
      "I’ve information vegetable, animal, and mineral,\n",
      "I know the kings of England, and I quote the ﬁghts historical\n",
      "From Marathon to Waterloo, in order categorical...\n",
      "Gilbert and Sullivan, Pirates of Penzance\n",
      "\n",
      "Imagine that you are an analyst with an investment ﬁrm that tracks airline stocks.\n",
      "You’re given the task of determining the relationship (if any) between airline an-\n",
      "nouncements of fare increases and the behavior of their stocks the next day. His-\n",
      "torical data about stock prices is easy to come by, but what about the airline an-\n",
      "nouncements? You will need to know at least the name of the airline, the nature of\n",
      "the proposed fare hike, the dates of the announcement, and possibly the response of\n",
      "other airlines. Fortunately, these can be all found in news articles \n"
     ]
    }
   ],
   "source": [
    "print(readPDF(infoExtractionBytes)[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we can either look at the full text or fiddle with our PDF reader and get more information about individual blocks of text.\n",
    "\n",
    "## Word Docs\n",
    "\n",
    "*NOTE* The package is called python-docx\n",
    "\n",
    "The other type of document you are likely to encounter is the `.docx`, these are actually a version of [XML](https://en.wikipedia.org/wiki/Office_Open_XML), just like HTML, and like HTML we will use a specialized parser.\n",
    "\n",
    "For this class we will use [`python-docx`](https://python-docx.readthedocs.io/en/latest/) which provides a nice simple interface for reading `.docx` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wenxi Xiao\n",
      "Professor Benjamin Soltoff\n",
      "MACS 30000\n",
      "10 October 2016\n",
      "Ethics of “Taste, Ties, and Time”\n",
      "\tIn 2008, sociologist Kevin Lewis and colleagues conduced a research project, entitled “Taste, Ties, and Time (3T),” to examine how race and cultural tastes could affect people’s social relationships. Researchers collected 1,640 college students’ Facebook profiles from Facebook.com and combined these data with the students’ school records, creating a new social network dataset. Through subsequent quantitative analyses of the dataset researchers concluded that gender, race, and socioeconomic status all influenced how these students behaved in social networks. In addition, the results showed that students’ Facebook friendships were correlated with their cultural preferences. Lewis and colleagues’ findings were published in the journal Social Networks and opened up a new branch in social science research. Despite its scientific potential, the project attracted critics from the public as well as the scientific community. The researchers were accused of violating research ethics, such that they scraped data without gaining informed consent and invading the students’ personal privacy by releasing the dataset to other researchers (Zimmer, 2010). Consequently, the students in the 3T project were identified, resulting a withdrawal of the dataset (Salganik, in open review). In this essay, I evaluate how the 3T project adheres to Salganik's four principles of ethical research, which are Respect for Persons, Beneficence, Justice, and Respect for Law and Public Interest.\n",
      "\tFirst of all, the 3T project violates the first principle, Respect for Persons, which is to ensure research participants’ autonomy (Salganik, in open review). According to the principle of Respect for Persons, people are entitled to be on their own free will to decide whether or not to participate in a given study. Additionally, special population such as children and prisoners are required to be treated with additional cautions. Researchers implement Respect for Persons by obtaining participants’ informed consent, a comprehensive written document allowing participants to be aware of the study’s procedures and potential risks. In the 3T project, researchers used students’ Facebook profiles data without the students’ awareness, not to mention obtaining informed consent. Such conduct clearly disobeyed the Respect for Persons principle. Although Lewis and colleagues claimed that when people registered for a Facebook account they agreed to allow Facebook to use their online data for research purposes by checking the term of use, it is still the researchers’ responsibility to explicitly give the students an opportunity to choose if they wanted their data to be included. After all, not many people carefully read all the items in term of use. Admittedly, violating the principle of Respect for Persons does not doom a study to a forbidden fate, but rather it alerts the researchers that their study needs amendments for being a more ethical one. \n"
     ]
    }
   ],
   "source": [
    "docxURL = 'https://github.com/xiaow2/persp-analysis/raw/02772bc5baf4044ba6410170ca740f14cd6155d5/assignments/short%20paper%201.docx'\n",
    "\n",
    "r = requests.get(docxURL, stream=True)\n",
    "d = docx.Document(io.BytesIO(r.content))\n",
    "for paragraph in d.paragraphs[:7]:\n",
    "    print(paragraph.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This procedure uses the `io.BytesIO` class again, since `docx.Document` expects a file. Another way to do it is to save document to a file then read it like any other file. If we do this we can either delete the file afterwords, or save it and skip downloading it next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadIfNeeded(targetURL, outputFile, **openkwargs):\n",
    "    if not os.path.isfile(outputFile):\n",
    "        outputDir = os.path.dirname(outputFile)\n",
    "        #This function is a more general os.mkdir()\n",
    "        os.makedirs(outputDir, exist_ok = True)\n",
    "        r = requests.get(targetURL, stream=True)\n",
    "        #Using a closure like this is generally better than having to\n",
    "        #remember to close the file. There are ways to make this function\n",
    "        #work as a closure too\n",
    "        with open(outputFile, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "    return open(outputFile, **openkwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will download, save and open `outputFile` as `outputFile` or just open it if `outputFile` exists. But by default `open()` will open the file as read only text with the local encoding. Which can cause issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadZipFile",
     "evalue": "File is not a zip file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-501099a20beb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownloadIfNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocxURL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data/temp.docx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/docx/api.py\u001b[0m in \u001b[0;36mDocument\u001b[0;34m(docx)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \"\"\"\n\u001b[1;32m     24\u001b[0m     \u001b[0mdocx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default_docx_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdocx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdocx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mdocument_part\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_document_part\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdocument_part\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent_type\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWML_DOCUMENT_MAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mtmpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"file '%s' is not a Word file, content type is '%s'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/docx/opc/package.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(cls, pkg_file)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0mpkg_file\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \"\"\"\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mpkg_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackageReader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0mpackage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mUnmarshaller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munmarshal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_reader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPartFactory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/docx/opc/pkgreader.py\u001b[0m in \u001b[0;36mfrom_file\u001b[0;34m(pkg_file)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mReturn\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m|\u001b[0m\u001b[0mPackageReader\u001b[0m\u001b[0;34m|\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mcontents\u001b[0m \u001b[0mof\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mpkg_file\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \"\"\"\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mphys_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPhysPkgReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mcontent_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ContentTypeMap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_xml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphys_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent_types_xml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mpkg_srels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackageReader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_srels_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphys_reader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPACKAGE_URI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/docx/opc/phys_pkg.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, pkg_file)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpkg_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ZipPkgReader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_zipf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mblob_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpack_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.5.1/Frameworks/Python.framework/Versions/3.5/lib/python3.5/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64)\u001b[0m\n\u001b[1;32m   1024\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RealGetContents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m                 \u001b[0;31m# set the modified flag so central directory gets written\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.5.1/Frameworks/Python.framework/Versions/3.5/lib/python3.5/zipfile.py\u001b[0m in \u001b[0;36m_RealGetContents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1091\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mendrec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBadZipFile\u001b[0m: File is not a zip file"
     ]
    }
   ],
   "source": [
    "d = docx.Document(downloadIfNeeded(docxURL, 'data/temp.docx'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to tell `open()` to read in binary mode (`'rb'`), this is why we added `**openkwargs`, this allows us to pass any keyword arguments (kwargs) from `downloadIfNeeded` to `open()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wenxi Xiao\n",
      "Professor Benjamin Soltoff\n",
      "MACS 30000\n",
      "10 October 2016\n",
      "Ethics of “Taste, Ties, and Time”\n",
      "\tIn 2008, sociologist Kevin Lewis and colleagues conduced a research project, entitled “Taste, Ties, and Time (3T),” to examine how race and cultural tastes could affect people’s social relationships. Researchers collected 1,640 college students’ Facebook profiles from Facebook.com and combined these data with the students’ school records, creating a new social network dataset. Through subsequent quantitative analyses of the dataset researchers concluded that gender, race, and socioeconomic status all influenced how these students behaved in social networks. In addition, the results showed that students’ Facebook friendships were correlated with their cultural preferences. Lewis and colleagues’ findings were published in the journal Social Networks and opened up a new branch in social science research. Despite its scientific potential, the project attracted critics from the public as well as the scientific community. The researchers were accused of violating research ethics, such that they scraped data without gaining informed consent and invading the students’ personal privacy by releasing the dataset to other researchers (Zimmer, 2010). Consequently, the students in the 3T project were identified, resulting a withdrawal of the dataset (Salganik, in open review). In this essay, I evaluate how the 3T project adheres to Salganik's four principles of ethical research, which are Respect for Persons, Beneficence, Justice, and Respect for Law and Public Interest.\n",
      "\tFirst of all, the 3T project violates the first principle, Respect for Persons, which is to ensure research participants’ autonomy (Salganik, in open review). According to the principle of Respect for Persons, people are entitled to be on their own free will to decide whether or not to participate in a given study. Additionally, special population such as children and prisoners are required to be treated with additional cautions. Researchers implement Respect for Persons by obtaining participants’ informed consent, a comprehensive written document allowing participants to be aware of the study’s procedures and potential risks. In the 3T project, researchers used students’ Facebook profiles data without the students’ awareness, not to mention obtaining informed consent. Such conduct clearly disobeyed the Respect for Persons principle. Although Lewis and colleagues claimed that when people registered for a Facebook account they agreed to allow Facebook to use their online data for research purposes by checking the term of use, it is still the researchers’ responsibility to explicitly give the students an opportunity to choose if they wanted their data to be included. After all, not many people carefully read all the items in term of use. Admittedly, violating the principle of Respect for Persons does not doom a study to a forbidden fate, but rather it alerts the researchers that their study needs amendments for being a more ethical one. \n"
     ]
    }
   ],
   "source": [
    "d = docx.Document(downloadIfNeeded(docxURL, 'data/temp.docx', mode = 'rb'))\n",
    "for paragraph in d.paragraphs[:7]:\n",
    "    print(paragraph.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can read the file with `docx.Document` and not have to wait for it to be downloaded every time."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
